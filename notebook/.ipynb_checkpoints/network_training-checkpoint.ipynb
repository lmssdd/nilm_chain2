{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6c3d26-334e-40e2-b5e7-b1a5fb4dc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba3b5da-a543-4e2c-9fd9-cc5b95fa40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 3., 30.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0b83-ea9e-4f00-adb4-a582e02fd6cc",
   "metadata": {},
   "source": [
    "### UKDale training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d621239f-ab4f-4281-8ea1-7d13667ea723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "houses=[1,2,5]\n",
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/ukdale/feather_files/UKDALE_%d_train.feather' %(i))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "  \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ba56dc-d29b-4258-8eeb-dccf7cc894ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(3)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(3)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(3)]\n",
    "\n",
    "ds_train = torch.utils.data.ConcatDataset([ds_house_train[0], ds_house_train[2]])\n",
    "ds_valid = torch.utils.data.ConcatDataset([ds_house_valid[0], ds_house_valid[2]])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_total[1], batch_size = 1, shuffle=False)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in [0,2]]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=True) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe51444-765c-47a9-82a5-ea7ecd25dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.56640 valid_loss: 0.45178 test_loss: 0.39688 \n",
      "Validation loss decreased (inf --> 0.451783).  Saving model ...\n",
      "[  2/100] train_loss: 0.38577 valid_loss: 0.34087 test_loss: 0.29473 \n",
      "Validation loss decreased (0.451783 --> 0.340872).  Saving model ...\n",
      "[  3/100] train_loss: 0.30618 valid_loss: 0.28832 test_loss: 0.24762 \n",
      "Validation loss decreased (0.340872 --> 0.288317).  Saving model ...\n",
      "[  4/100] train_loss: 0.26508 valid_loss: 0.25632 test_loss: 0.21095 \n",
      "Validation loss decreased (0.288317 --> 0.256318).  Saving model ...\n",
      "[  5/100] train_loss: 0.23441 valid_loss: 0.23656 test_loss: 0.18588 \n",
      "Validation loss decreased (0.256318 --> 0.236562).  Saving model ...\n",
      "[  6/100] train_loss: 0.21302 valid_loss: 0.22298 test_loss: 0.17206 \n",
      "Validation loss decreased (0.236562 --> 0.222982).  Saving model ...\n",
      "[  7/100] train_loss: 0.19793 valid_loss: 0.20521 test_loss: 0.15555 \n",
      "Validation loss decreased (0.222982 --> 0.205215).  Saving model ...\n",
      "[  8/100] train_loss: 0.18838 valid_loss: 0.19760 test_loss: 0.14786 \n",
      "Validation loss decreased (0.205215 --> 0.197597).  Saving model ...\n",
      "[  9/100] train_loss: 0.17926 valid_loss: 0.18732 test_loss: 0.13980 \n",
      "Validation loss decreased (0.197597 --> 0.187324).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17536 valid_loss: 0.18739 test_loss: 0.13584 \n",
      "[ 11/100] train_loss: 0.16589 valid_loss: 0.17708 test_loss: 0.12865 \n",
      "Validation loss decreased (0.187324 --> 0.177079).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16092 valid_loss: 0.17702 test_loss: 0.12466 \n",
      "Validation loss decreased (0.177079 --> 0.177018).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15725 valid_loss: 0.17169 test_loss: 0.12247 \n",
      "Validation loss decreased (0.177018 --> 0.171690).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15157 valid_loss: 0.16998 test_loss: 0.12264 \n",
      "Validation loss decreased (0.171690 --> 0.169985).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15498 valid_loss: 0.16773 test_loss: 0.12172 \n",
      "Validation loss decreased (0.169985 --> 0.167734).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14842 valid_loss: 0.16447 test_loss: 0.11777 \n",
      "Validation loss decreased (0.167734 --> 0.164469).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14567 valid_loss: 0.16777 test_loss: 0.12248 \n",
      "[ 18/100] train_loss: 0.14549 valid_loss: 0.16266 test_loss: 0.11608 \n",
      "Validation loss decreased (0.164469 --> 0.162662).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14053 valid_loss: 0.16175 test_loss: 0.11641 \n",
      "Validation loss decreased (0.162662 --> 0.161754).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14324 valid_loss: 0.15529 test_loss: 0.11267 \n",
      "Validation loss decreased (0.161754 --> 0.155289).  Saving model ...\n",
      "[ 21/100] train_loss: 0.14113 valid_loss: 0.15582 test_loss: 0.11366 \n",
      "[ 22/100] train_loss: 0.13378 valid_loss: 0.15359 test_loss: 0.11179 \n",
      "Validation loss decreased (0.155289 --> 0.153585).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13493 valid_loss: 0.15157 test_loss: 0.10956 \n",
      "Validation loss decreased (0.153585 --> 0.151574).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13322 valid_loss: 0.15206 test_loss: 0.11077 \n",
      "[ 25/100] train_loss: 0.13029 valid_loss: 0.14563 test_loss: 0.10451 \n",
      "Validation loss decreased (0.151574 --> 0.145629).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13296 valid_loss: 0.15040 test_loss: 0.10994 \n",
      "[ 27/100] train_loss: 0.12690 valid_loss: 0.14310 test_loss: 0.10448 \n",
      "Validation loss decreased (0.145629 --> 0.143101).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12907 valid_loss: 0.14754 test_loss: 0.10657 \n",
      "[ 29/100] train_loss: 0.12791 valid_loss: 0.14661 test_loss: 0.10798 \n",
      "[ 30/100] train_loss: 0.12281 valid_loss: 0.14704 test_loss: 0.10958 \n",
      "[ 31/100] train_loss: 0.12282 valid_loss: 0.13877 test_loss: 0.10000 \n",
      "Validation loss decreased (0.143101 --> 0.138774).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12137 valid_loss: 0.14564 test_loss: 0.10157 \n",
      "[ 33/100] train_loss: 0.12573 valid_loss: 0.13804 test_loss: 0.09988 \n",
      "Validation loss decreased (0.138774 --> 0.138044).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12145 valid_loss: 0.13997 test_loss: 0.09967 \n",
      "[ 35/100] train_loss: 0.12091 valid_loss: 0.13838 test_loss: 0.10077 \n",
      "[ 36/100] train_loss: 0.11967 valid_loss: 0.13807 test_loss: 0.10176 \n",
      "[ 37/100] train_loss: 0.12366 valid_loss: 0.13518 test_loss: 0.09925 \n",
      "Validation loss decreased (0.138044 --> 0.135182).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11781 valid_loss: 0.13607 test_loss: 0.09952 \n",
      "[ 39/100] train_loss: 0.11539 valid_loss: 0.13492 test_loss: 0.10010 \n",
      "Validation loss decreased (0.135182 --> 0.134917).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11681 valid_loss: 0.13563 test_loss: 0.09832 \n",
      "[ 41/100] train_loss: 0.11623 valid_loss: 0.13198 test_loss: 0.09604 \n",
      "Validation loss decreased (0.134917 --> 0.131978).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11549 valid_loss: 0.13231 test_loss: 0.09600 \n",
      "[ 43/100] train_loss: 0.11536 valid_loss: 0.13085 test_loss: 0.09731 \n",
      "Validation loss decreased (0.131978 --> 0.130849).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11197 valid_loss: 0.13458 test_loss: 0.10032 \n",
      "[ 45/100] train_loss: 0.11179 valid_loss: 0.13413 test_loss: 0.10091 \n",
      "[ 46/100] train_loss: 0.11318 valid_loss: 0.13141 test_loss: 0.09437 \n",
      "[ 47/100] train_loss: 0.11227 valid_loss: 0.12892 test_loss: 0.09528 \n",
      "Validation loss decreased (0.130849 --> 0.128920).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11339 valid_loss: 0.13065 test_loss: 0.09504 \n",
      "[ 49/100] train_loss: 0.11150 valid_loss: 0.13029 test_loss: 0.09799 \n",
      "[ 50/100] train_loss: 0.11256 valid_loss: 0.13027 test_loss: 0.09690 \n",
      "[ 51/100] train_loss: 0.11521 valid_loss: 0.12740 test_loss: 0.09578 \n",
      "Validation loss decreased (0.128920 --> 0.127397).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10774 valid_loss: 0.12892 test_loss: 0.09634 \n",
      "[ 53/100] train_loss: 0.11036 valid_loss: 0.12951 test_loss: 0.09737 \n",
      "[ 54/100] train_loss: 0.10876 valid_loss: 0.12814 test_loss: 0.09495 \n",
      "[ 55/100] train_loss: 0.10515 valid_loss: 0.12380 test_loss: 0.09249 \n",
      "Validation loss decreased (0.127397 --> 0.123800).  Saving model ...\n",
      "[ 56/100] train_loss: 0.11145 valid_loss: 0.12258 test_loss: 0.09292 \n",
      "Validation loss decreased (0.123800 --> 0.122576).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10772 valid_loss: 0.12758 test_loss: 0.09649 \n",
      "[ 58/100] train_loss: 0.10755 valid_loss: 0.12552 test_loss: 0.09372 \n",
      "[ 59/100] train_loss: 0.10958 valid_loss: 0.12416 test_loss: 0.09375 \n",
      "[ 60/100] train_loss: 0.10585 valid_loss: 0.12623 test_loss: 0.09476 \n",
      "[ 61/100] train_loss: 0.10843 valid_loss: 0.12452 test_loss: 0.09337 \n",
      "[ 62/100] train_loss: 0.10158 valid_loss: 0.12443 test_loss: 0.09539 \n",
      "[ 63/100] train_loss: 0.10237 valid_loss: 0.12770 test_loss: 0.09698 \n",
      "[ 64/100] train_loss: 0.10279 valid_loss: 0.12119 test_loss: 0.09107 \n",
      "Validation loss decreased (0.122576 --> 0.121191).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10476 valid_loss: 0.12250 test_loss: 0.09334 \n",
      "[ 66/100] train_loss: 0.10534 valid_loss: 0.12361 test_loss: 0.09504 \n",
      "[ 67/100] train_loss: 0.10226 valid_loss: 0.12179 test_loss: 0.09161 \n",
      "[ 68/100] train_loss: 0.10195 valid_loss: 0.12383 test_loss: 0.09246 \n",
      "[ 69/100] train_loss: 0.10471 valid_loss: 0.12018 test_loss: 0.09211 \n",
      "Validation loss decreased (0.121191 --> 0.120185).  Saving model ...\n",
      "[ 70/100] train_loss: 0.10198 valid_loss: 0.11993 test_loss: 0.09086 \n",
      "Validation loss decreased (0.120185 --> 0.119930).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10647 valid_loss: 0.11957 test_loss: 0.09232 \n",
      "Validation loss decreased (0.119930 --> 0.119568).  Saving model ...\n",
      "[ 72/100] train_loss: 0.10246 valid_loss: 0.11930 test_loss: 0.09362 \n",
      "Validation loss decreased (0.119568 --> 0.119299).  Saving model ...\n",
      "[ 73/100] train_loss: 0.10187 valid_loss: 0.12175 test_loss: 0.09230 \n",
      "[ 74/100] train_loss: 0.10045 valid_loss: 0.11965 test_loss: 0.09135 \n",
      "[ 75/100] train_loss: 0.10097 valid_loss: 0.12189 test_loss: 0.09745 \n",
      "[ 76/100] train_loss: 0.09989 valid_loss: 0.11862 test_loss: 0.09197 \n",
      "Validation loss decreased (0.119299 --> 0.118624).  Saving model ...\n",
      "[ 77/100] train_loss: 0.10170 valid_loss: 0.11882 test_loss: 0.09135 \n",
      "[ 78/100] train_loss: 0.10029 valid_loss: 0.11729 test_loss: 0.08879 \n",
      "Validation loss decreased (0.118624 --> 0.117292).  Saving model ...\n",
      "[ 79/100] train_loss: 0.10254 valid_loss: 0.11630 test_loss: 0.08798 \n",
      "Validation loss decreased (0.117292 --> 0.116301).  Saving model ...\n",
      "[ 80/100] train_loss: 0.10065 valid_loss: 0.11510 test_loss: 0.08610 \n",
      "Validation loss decreased (0.116301 --> 0.115104).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09806 valid_loss: 0.11940 test_loss: 0.09174 \n",
      "[ 82/100] train_loss: 0.09715 valid_loss: 0.11341 test_loss: 0.08855 \n",
      "Validation loss decreased (0.115104 --> 0.113413).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09486 valid_loss: 0.11493 test_loss: 0.08772 \n",
      "[ 84/100] train_loss: 0.10004 valid_loss: 0.11699 test_loss: 0.09061 \n",
      "[ 85/100] train_loss: 0.10093 valid_loss: 0.11528 test_loss: 0.09047 \n",
      "[ 86/100] train_loss: 0.09935 valid_loss: 0.11524 test_loss: 0.08895 \n",
      "[ 87/100] train_loss: 0.09930 valid_loss: 0.11541 test_loss: 0.09049 \n",
      "[ 88/100] train_loss: 0.09726 valid_loss: 0.11642 test_loss: 0.09141 \n",
      "[ 89/100] train_loss: 0.09376 valid_loss: 0.11443 test_loss: 0.08861 \n",
      "[ 90/100] train_loss: 0.09718 valid_loss: 0.11462 test_loss: 0.09011 \n",
      "[ 91/100] train_loss: 0.09666 valid_loss: 0.11452 test_loss: 0.08794 \n",
      "[ 92/100] train_loss: 0.09736 valid_loss: 0.11307 test_loss: 0.08795 \n",
      "Validation loss decreased (0.113413 --> 0.113068).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09704 valid_loss: 0.11445 test_loss: 0.08885 \n",
      "[ 94/100] train_loss: 0.09809 valid_loss: 0.11270 test_loss: 0.08700 \n",
      "Validation loss decreased (0.113068 --> 0.112705).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09099 valid_loss: 0.11202 test_loss: 0.08779 \n",
      "Validation loss decreased (0.112705 --> 0.112018).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09347 valid_loss: 0.11708 test_loss: 0.09605 \n",
      "[ 97/100] train_loss: 0.09306 valid_loss: 0.11501 test_loss: 0.08900 \n",
      "[ 98/100] train_loss: 0.09297 valid_loss: 0.11316 test_loss: 0.08845 \n",
      "[ 99/100] train_loss: 0.09453 valid_loss: 0.11043 test_loss: 0.08542 \n",
      "Validation loss decreased (0.112018 --> 0.110425).  Saving model ...\n",
      "[100/100] train_loss: 0.09419 valid_loss: 0.11221 test_loss: 0.08993 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "model = PTPNet(1,3,32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "fn='../data/ukdale/network_weights/UKDALE_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cac8f1-b7b1-4e0c-996e-c49a672e5932",
   "metadata": {},
   "source": [
    "### REFIT training network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a85407-aa8c-47d2-9717-7cd596c1579b",
   "metadata": {},
   "source": [
    "##### Fridge case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f3726e-9dc1-4e33-9020-3a95333eb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge']\n",
    "THRESHOLD = [50.]\n",
    "MIN_ON = [1.]\n",
    "MIN_OFF = [1.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b8dee4-cd29-47be-913e-2039a4d93d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses=[2,5,9,12,15]\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/fridge/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1210c891-39d9-4b45-a69e-154fd67c2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i],SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5)]\n",
    "\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_total[0], \n",
    "                                                ds_house_total[1], \n",
    "                                                ds_house_total[2]])\n",
    "ds_valid       = torch.utils.data.ConcatDataset([ds_house_total[3]])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(dataset = ds_house_total[4], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_total[4], batch_size = 1, shuffle=False)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_total[3], batch_size = 1, shuffle=False)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "067a4125-0716-48ba-988b-c5a702ff2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.60252 valid_loss: 0.61343 test_loss: 0.54161 \n",
      "Validation loss decreased (inf --> 0.613428).  Saving model ...\n",
      "[  2/100] train_loss: 0.51198 valid_loss: 0.58967 test_loss: 0.44051 \n",
      "Validation loss decreased (0.613428 --> 0.589669).  Saving model ...\n",
      "[  3/100] train_loss: 0.47456 valid_loss: 0.62230 test_loss: 0.45843 \n",
      "[  4/100] train_loss: 0.45394 valid_loss: 0.60215 test_loss: 0.46900 \n",
      "[  5/100] train_loss: 0.44118 valid_loss: 0.67261 test_loss: 0.58120 \n",
      "[  6/100] train_loss: 0.42975 valid_loss: 0.66753 test_loss: 0.57120 \n",
      "[  7/100] train_loss: 0.41930 valid_loss: 0.62104 test_loss: 0.47636 \n",
      "[  8/100] train_loss: 0.41206 valid_loss: 0.69335 test_loss: 0.58430 \n",
      "[  9/100] train_loss: 0.40404 valid_loss: 0.68225 test_loss: 0.54839 \n",
      "[ 10/100] train_loss: 0.39878 valid_loss: 0.76673 test_loss: 0.66672 \n",
      "[ 11/100] train_loss: 0.39137 valid_loss: 0.76341 test_loss: 0.57578 \n",
      "[ 12/100] train_loss: 0.38679 valid_loss: 0.84663 test_loss: 0.63659 \n",
      "[ 13/100] train_loss: 0.38211 valid_loss: 0.75813 test_loss: 0.62185 \n",
      "[ 14/100] train_loss: 0.37638 valid_loss: 0.79018 test_loss: 0.55144 \n",
      "[ 15/100] train_loss: 0.37326 valid_loss: 0.73143 test_loss: 0.55710 \n",
      "[ 16/100] train_loss: 0.36973 valid_loss: 0.78858 test_loss: 0.66146 \n",
      "[ 17/100] train_loss: 0.36702 valid_loss: 0.73812 test_loss: 0.59169 \n",
      "[ 18/100] train_loss: 0.36427 valid_loss: 0.76885 test_loss: 0.51331 \n",
      "[ 19/100] train_loss: 0.36202 valid_loss: 0.72178 test_loss: 0.46679 \n",
      "[ 20/100] train_loss: 0.35817 valid_loss: 0.79372 test_loss: 0.57879 \n",
      "[ 21/100] train_loss: 0.35408 valid_loss: 0.65149 test_loss: 0.45063 \n",
      "[ 22/100] train_loss: 0.35168 valid_loss: 0.70171 test_loss: 0.41671 \n",
      "[ 23/100] train_loss: 0.34753 valid_loss: 0.68808 test_loss: 0.48467 \n",
      "[ 24/100] train_loss: 0.34377 valid_loss: 0.70290 test_loss: 0.43032 \n",
      "[ 25/100] train_loss: 0.34383 valid_loss: 0.62265 test_loss: 0.37146 \n",
      "[ 26/100] train_loss: 0.34302 valid_loss: 0.65462 test_loss: 0.36407 \n",
      "[ 27/100] train_loss: 0.33830 valid_loss: 0.67611 test_loss: 0.38984 \n",
      "[ 28/100] train_loss: 0.33579 valid_loss: 0.67235 test_loss: 0.43006 \n",
      "[ 29/100] train_loss: 0.33475 valid_loss: 0.58196 test_loss: 0.36515 \n",
      "Validation loss decreased (0.589669 --> 0.581961).  Saving model ...\n",
      "[ 30/100] train_loss: 0.33198 valid_loss: 0.70642 test_loss: 0.41174 \n",
      "[ 31/100] train_loss: 0.32923 valid_loss: 0.65137 test_loss: 0.37037 \n",
      "[ 32/100] train_loss: 0.32819 valid_loss: 0.66995 test_loss: 0.41670 \n",
      "[ 33/100] train_loss: 0.32655 valid_loss: 0.60175 test_loss: 0.34108 \n",
      "[ 34/100] train_loss: 0.32448 valid_loss: 0.64056 test_loss: 0.37280 \n",
      "[ 35/100] train_loss: 0.32200 valid_loss: 0.56482 test_loss: 0.34845 \n",
      "Validation loss decreased (0.581961 --> 0.564822).  Saving model ...\n",
      "[ 36/100] train_loss: 0.32172 valid_loss: 0.63795 test_loss: 0.35501 \n",
      "[ 37/100] train_loss: 0.31861 valid_loss: 0.71117 test_loss: 0.39312 \n",
      "[ 38/100] train_loss: 0.31917 valid_loss: 0.60030 test_loss: 0.34322 \n",
      "[ 39/100] train_loss: 0.31610 valid_loss: 0.77217 test_loss: 0.40014 \n",
      "[ 40/100] train_loss: 0.31471 valid_loss: 0.68840 test_loss: 0.36657 \n",
      "[ 41/100] train_loss: 0.31292 valid_loss: 0.57121 test_loss: 0.35486 \n",
      "[ 42/100] train_loss: 0.31164 valid_loss: 0.68012 test_loss: 0.35603 \n",
      "[ 43/100] train_loss: 0.31113 valid_loss: 0.69577 test_loss: 0.34447 \n",
      "[ 44/100] train_loss: 0.31088 valid_loss: 0.67166 test_loss: 0.35268 \n",
      "[ 45/100] train_loss: 0.30794 valid_loss: 0.68542 test_loss: 0.34806 \n",
      "[ 46/100] train_loss: 0.30573 valid_loss: 0.66493 test_loss: 0.37201 \n",
      "[ 47/100] train_loss: 0.30602 valid_loss: 0.59986 test_loss: 0.32850 \n",
      "[ 48/100] train_loss: 0.30346 valid_loss: 0.64796 test_loss: 0.35099 \n",
      "[ 49/100] train_loss: 0.30370 valid_loss: 0.60726 test_loss: 0.31649 \n",
      "[ 50/100] train_loss: 0.30193 valid_loss: 0.57337 test_loss: 0.36132 \n",
      "[ 51/100] train_loss: 0.30078 valid_loss: 0.66634 test_loss: 0.36789 \n",
      "[ 52/100] train_loss: 0.30057 valid_loss: 0.65023 test_loss: 0.37070 \n",
      "[ 53/100] train_loss: 0.29931 valid_loss: 0.67593 test_loss: 0.37367 \n",
      "[ 54/100] train_loss: 0.29887 valid_loss: 0.61199 test_loss: 0.32063 \n",
      "[ 55/100] train_loss: 0.29897 valid_loss: 0.58598 test_loss: 0.35480 \n",
      "[ 56/100] train_loss: 0.29729 valid_loss: 0.61180 test_loss: 0.33150 \n",
      "[ 57/100] train_loss: 0.29526 valid_loss: 0.67940 test_loss: 0.34500 \n",
      "[ 58/100] train_loss: 0.29498 valid_loss: 0.61020 test_loss: 0.33903 \n",
      "[ 59/100] train_loss: 0.29374 valid_loss: 0.73315 test_loss: 0.37789 \n",
      "[ 60/100] train_loss: 0.29105 valid_loss: 0.69987 test_loss: 0.35027 \n",
      "[ 61/100] train_loss: 0.29052 valid_loss: 0.68558 test_loss: 0.33497 \n",
      "[ 62/100] train_loss: 0.28864 valid_loss: 0.67769 test_loss: 0.36330 \n",
      "[ 63/100] train_loss: 0.28932 valid_loss: 0.56655 test_loss: 0.33395 \n",
      "[ 64/100] train_loss: 0.28844 valid_loss: 0.67558 test_loss: 0.35079 \n",
      "[ 65/100] train_loss: 0.28827 valid_loss: 0.70054 test_loss: 0.34284 \n",
      "[ 66/100] train_loss: 0.28642 valid_loss: 0.64257 test_loss: 0.35003 \n",
      "[ 67/100] train_loss: 0.28735 valid_loss: 0.71105 test_loss: 0.34335 \n",
      "[ 68/100] train_loss: 0.28405 valid_loss: 0.65930 test_loss: 0.33414 \n",
      "[ 69/100] train_loss: 0.28327 valid_loss: 0.61151 test_loss: 0.35228 \n",
      "[ 70/100] train_loss: 0.28415 valid_loss: 0.67841 test_loss: 0.34373 \n",
      "[ 71/100] train_loss: 0.28326 valid_loss: 0.59447 test_loss: 0.33668 \n",
      "[ 72/100] train_loss: 0.28181 valid_loss: 0.65332 test_loss: 0.33472 \n",
      "[ 73/100] train_loss: 0.28159 valid_loss: 0.67630 test_loss: 0.34251 \n",
      "[ 74/100] train_loss: 0.28004 valid_loss: 0.63881 test_loss: 0.37563 \n",
      "[ 75/100] train_loss: 0.27935 valid_loss: 0.63000 test_loss: 0.35464 \n",
      "[ 76/100] train_loss: 0.27973 valid_loss: 0.72190 test_loss: 0.37132 \n",
      "[ 77/100] train_loss: 0.27818 valid_loss: 0.61709 test_loss: 0.33922 \n",
      "[ 78/100] train_loss: 0.27695 valid_loss: 0.63205 test_loss: 0.32680 \n",
      "[ 79/100] train_loss: 0.27756 valid_loss: 0.65749 test_loss: 0.34900 \n",
      "[ 80/100] train_loss: 0.27639 valid_loss: 0.66574 test_loss: 0.36153 \n",
      "[ 81/100] train_loss: 0.27594 valid_loss: 0.61249 test_loss: 0.33693 \n",
      "[ 82/100] train_loss: 0.27512 valid_loss: 0.72899 test_loss: 0.39804 \n",
      "[ 83/100] train_loss: 0.27493 valid_loss: 0.64599 test_loss: 0.34702 \n",
      "[ 84/100] train_loss: 0.27296 valid_loss: 0.66691 test_loss: 0.36478 \n",
      "[ 85/100] train_loss: 0.27347 valid_loss: 0.67172 test_loss: 0.35734 \n",
      "[ 86/100] train_loss: 0.27336 valid_loss: 0.58370 test_loss: 0.33077 \n",
      "[ 87/100] train_loss: 0.27223 valid_loss: 0.68504 test_loss: 0.34630 \n",
      "[ 88/100] train_loss: 0.27057 valid_loss: 0.61583 test_loss: 0.33332 \n",
      "[ 89/100] train_loss: 0.26993 valid_loss: 0.64685 test_loss: 0.33892 \n",
      "[ 90/100] train_loss: 0.27059 valid_loss: 0.67885 test_loss: 0.33217 \n",
      "[ 91/100] train_loss: 0.27020 valid_loss: 0.68774 test_loss: 0.36359 \n",
      "[ 92/100] train_loss: 0.26826 valid_loss: 0.75596 test_loss: 0.36568 \n",
      "[ 93/100] train_loss: 0.26770 valid_loss: 0.68985 test_loss: 0.35257 \n",
      "[ 94/100] train_loss: 0.26818 valid_loss: 0.59455 test_loss: 0.31423 \n",
      "[ 95/100] train_loss: 0.26749 valid_loss: 0.72064 test_loss: 0.35169 \n",
      "[ 96/100] train_loss: 0.26697 valid_loss: 0.75607 test_loss: 0.38385 \n",
      "[ 97/100] train_loss: 0.26603 valid_loss: 0.65742 test_loss: 0.35212 \n",
      "[ 98/100] train_loss: 0.26465 valid_loss: 0.73079 test_loss: 0.36852 \n",
      "[ 99/100] train_loss: 0.26445 valid_loss: 0.62803 test_loss: 0.34025 \n",
      "[100/100] train_loss: 0.26634 valid_loss: 0.69731 test_loss: 0.36002 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "fn = '../data/refit/network_weights/REFIT_fridge_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbe634-fca1-47aa-8ca3-46a220582dae",
   "metadata": {},
   "source": [
    "##### Dishwasher case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcb0c38-d8ff-4c3f-af11-00957bb64835",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['dishwasher']\n",
    "THRESHOLD = [100.]\n",
    "MIN_ON = [30.]\n",
    "MIN_OFF = [90.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49035a44-156d-49fe-a18c-57f909dde269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses=[5,7,9,13,16,18,20]\n",
    "\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/dishwasher/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5142a81-8980-4e1b-a83b-a900467b94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i],SEQ_LEN, BORDER, MAX_POWER, False) for i in range(7)]\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_total[i] for i in range(5)])\n",
    "ds_valid       = torch.utils.data.ConcatDataset([ds_house_total[5]])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(dataset = ds_house_total[6], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c95155d-2d79-45f1-8429-f29bea064164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.23210 valid_loss: 0.06540 test_loss: 0.05800 \n",
      "Validation loss decreased (inf --> 0.065397).  Saving model ...\n",
      "[  2/100] train_loss: 0.11197 valid_loss: 0.03870 test_loss: 0.03439 \n",
      "Validation loss decreased (0.065397 --> 0.038697).  Saving model ...\n",
      "[  3/100] train_loss: 0.09023 valid_loss: 0.04099 test_loss: 0.03088 \n",
      "[  4/100] train_loss: 0.08082 valid_loss: 0.04284 test_loss: 0.02929 \n",
      "[  5/100] train_loss: 0.07443 valid_loss: 0.04346 test_loss: 0.02831 \n",
      "[  6/100] train_loss: 0.07041 valid_loss: 0.04546 test_loss: 0.02919 \n",
      "[  7/100] train_loss: 0.06738 valid_loss: 0.04658 test_loss: 0.02917 \n",
      "[  8/100] train_loss: 0.06441 valid_loss: 0.04759 test_loss: 0.03028 \n",
      "[  9/100] train_loss: 0.06195 valid_loss: 0.04972 test_loss: 0.03037 \n",
      "[ 10/100] train_loss: 0.05985 valid_loss: 0.04843 test_loss: 0.02981 \n",
      "[ 11/100] train_loss: 0.05782 valid_loss: 0.05004 test_loss: 0.03010 \n",
      "[ 12/100] train_loss: 0.05621 valid_loss: 0.05071 test_loss: 0.02990 \n",
      "[ 13/100] train_loss: 0.05502 valid_loss: 0.05176 test_loss: 0.03033 \n",
      "[ 14/100] train_loss: 0.05423 valid_loss: 0.05170 test_loss: 0.03059 \n",
      "[ 15/100] train_loss: 0.05299 valid_loss: 0.05278 test_loss: 0.03093 \n",
      "[ 16/100] train_loss: 0.05118 valid_loss: 0.05363 test_loss: 0.03072 \n",
      "[ 17/100] train_loss: 0.05069 valid_loss: 0.05340 test_loss: 0.02984 \n",
      "[ 18/100] train_loss: 0.05023 valid_loss: 0.05331 test_loss: 0.03048 \n",
      "[ 19/100] train_loss: 0.04879 valid_loss: 0.05263 test_loss: 0.03064 \n",
      "[ 20/100] train_loss: 0.04822 valid_loss: 0.05513 test_loss: 0.03190 \n",
      "[ 21/100] train_loss: 0.04742 valid_loss: 0.05710 test_loss: 0.03186 \n",
      "[ 22/100] train_loss: 0.04646 valid_loss: 0.05587 test_loss: 0.03144 \n",
      "[ 23/100] train_loss: 0.04555 valid_loss: 0.05911 test_loss: 0.03250 \n",
      "[ 24/100] train_loss: 0.04509 valid_loss: 0.05847 test_loss: 0.03226 \n",
      "[ 25/100] train_loss: 0.04430 valid_loss: 0.05869 test_loss: 0.03322 \n",
      "[ 26/100] train_loss: 0.04330 valid_loss: 0.05991 test_loss: 0.03410 \n",
      "[ 27/100] train_loss: 0.04281 valid_loss: 0.05986 test_loss: 0.03447 \n",
      "[ 28/100] train_loss: 0.04214 valid_loss: 0.06218 test_loss: 0.03395 \n",
      "[ 29/100] train_loss: 0.04159 valid_loss: 0.06131 test_loss: 0.03519 \n",
      "[ 30/100] train_loss: 0.04077 valid_loss: 0.06042 test_loss: 0.03496 \n",
      "[ 31/100] train_loss: 0.04012 valid_loss: 0.06333 test_loss: 0.03561 \n",
      "[ 32/100] train_loss: 0.03958 valid_loss: 0.06378 test_loss: 0.03450 \n",
      "[ 33/100] train_loss: 0.03876 valid_loss: 0.06071 test_loss: 0.03560 \n",
      "[ 34/100] train_loss: 0.03784 valid_loss: 0.06155 test_loss: 0.03520 \n",
      "[ 35/100] train_loss: 0.03744 valid_loss: 0.06099 test_loss: 0.03546 \n",
      "[ 36/100] train_loss: 0.03654 valid_loss: 0.06199 test_loss: 0.03580 \n",
      "[ 37/100] train_loss: 0.03597 valid_loss: 0.05976 test_loss: 0.03381 \n",
      "[ 38/100] train_loss: 0.03544 valid_loss: 0.06090 test_loss: 0.03494 \n",
      "[ 39/100] train_loss: 0.03502 valid_loss: 0.06146 test_loss: 0.03575 \n",
      "[ 40/100] train_loss: 0.03447 valid_loss: 0.06375 test_loss: 0.03717 \n",
      "[ 41/100] train_loss: 0.03417 valid_loss: 0.06340 test_loss: 0.03711 \n",
      "[ 42/100] train_loss: 0.03273 valid_loss: 0.06470 test_loss: 0.03732 \n",
      "[ 43/100] train_loss: 0.03210 valid_loss: 0.06786 test_loss: 0.03690 \n",
      "[ 44/100] train_loss: 0.03141 valid_loss: 0.06617 test_loss: 0.03603 \n",
      "[ 45/100] train_loss: 0.03104 valid_loss: 0.06542 test_loss: 0.03664 \n",
      "[ 46/100] train_loss: 0.02984 valid_loss: 0.06578 test_loss: 0.03509 \n",
      "[ 47/100] train_loss: 0.02967 valid_loss: 0.06529 test_loss: 0.03699 \n",
      "[ 48/100] train_loss: 0.02919 valid_loss: 0.06790 test_loss: 0.03780 \n",
      "[ 49/100] train_loss: 0.02854 valid_loss: 0.06516 test_loss: 0.03610 \n",
      "[ 50/100] train_loss: 0.02769 valid_loss: 0.06887 test_loss: 0.03803 \n",
      "[ 51/100] train_loss: 0.02690 valid_loss: 0.06990 test_loss: 0.03802 \n",
      "[ 52/100] train_loss: 0.02677 valid_loss: 0.07447 test_loss: 0.03730 \n",
      "[ 53/100] train_loss: 0.02642 valid_loss: 0.07042 test_loss: 0.03749 \n",
      "[ 54/100] train_loss: 0.02561 valid_loss: 0.07530 test_loss: 0.04030 \n",
      "[ 55/100] train_loss: 0.02539 valid_loss: 0.07071 test_loss: 0.03590 \n",
      "[ 56/100] train_loss: 0.02469 valid_loss: 0.07165 test_loss: 0.03831 \n",
      "[ 57/100] train_loss: 0.02435 valid_loss: 0.07962 test_loss: 0.04688 \n",
      "[ 58/100] train_loss: 0.02426 valid_loss: 0.07443 test_loss: 0.04082 \n",
      "[ 59/100] train_loss: 0.02323 valid_loss: 0.08579 test_loss: 0.04805 \n",
      "[ 60/100] train_loss: 0.02293 valid_loss: 0.08039 test_loss: 0.04051 \n",
      "[ 61/100] train_loss: 0.02207 valid_loss: 0.08465 test_loss: 0.04557 \n",
      "[ 62/100] train_loss: 0.02194 valid_loss: 0.08012 test_loss: 0.04318 \n",
      "[ 63/100] train_loss: 0.02096 valid_loss: 0.07734 test_loss: 0.03954 \n",
      "[ 64/100] train_loss: 0.02077 valid_loss: 0.07944 test_loss: 0.04344 \n",
      "[ 65/100] train_loss: 0.02104 valid_loss: 0.07949 test_loss: 0.04620 \n",
      "[ 66/100] train_loss: 0.02062 valid_loss: 0.07823 test_loss: 0.04289 \n",
      "[ 67/100] train_loss: 0.02035 valid_loss: 0.08677 test_loss: 0.04317 \n",
      "[ 68/100] train_loss: 0.02009 valid_loss: 0.08749 test_loss: 0.04564 \n",
      "[ 69/100] train_loss: 0.01942 valid_loss: 0.09924 test_loss: 0.05770 \n",
      "[ 70/100] train_loss: 0.01927 valid_loss: 0.08392 test_loss: 0.04988 \n",
      "[ 71/100] train_loss: 0.01850 valid_loss: 0.10070 test_loss: 0.05253 \n",
      "[ 72/100] train_loss: 0.01826 valid_loss: 0.08500 test_loss: 0.04629 \n",
      "[ 73/100] train_loss: 0.01826 valid_loss: 0.08760 test_loss: 0.04936 \n",
      "[ 74/100] train_loss: 0.01767 valid_loss: 0.09089 test_loss: 0.04729 \n",
      "[ 75/100] train_loss: 0.01819 valid_loss: 0.09180 test_loss: 0.05298 \n",
      "[ 76/100] train_loss: 0.01708 valid_loss: 0.08754 test_loss: 0.04963 \n",
      "[ 77/100] train_loss: 0.01694 valid_loss: 0.11250 test_loss: 0.06315 \n",
      "[ 78/100] train_loss: 0.01659 valid_loss: 0.10814 test_loss: 0.05169 \n",
      "[ 79/100] train_loss: 0.01660 valid_loss: 0.10859 test_loss: 0.05107 \n",
      "[ 80/100] train_loss: 0.01618 valid_loss: 0.10454 test_loss: 0.06162 \n",
      "[ 81/100] train_loss: 0.01577 valid_loss: 0.10823 test_loss: 0.05743 \n",
      "[ 82/100] train_loss: 0.01563 valid_loss: 0.10954 test_loss: 0.05789 \n",
      "[ 83/100] train_loss: 0.01541 valid_loss: 0.11668 test_loss: 0.05937 \n",
      "[ 84/100] train_loss: 0.01537 valid_loss: 0.09652 test_loss: 0.05066 \n",
      "[ 85/100] train_loss: 0.01498 valid_loss: 0.10326 test_loss: 0.06186 \n",
      "[ 86/100] train_loss: 0.01486 valid_loss: 0.10958 test_loss: 0.05626 \n",
      "[ 87/100] train_loss: 0.01481 valid_loss: 0.11234 test_loss: 0.06474 \n",
      "[ 88/100] train_loss: 0.01439 valid_loss: 0.11285 test_loss: 0.06102 \n",
      "[ 89/100] train_loss: 0.01444 valid_loss: 0.11128 test_loss: 0.06000 \n",
      "[ 90/100] train_loss: 0.01396 valid_loss: 0.14158 test_loss: 0.06920 \n",
      "[ 91/100] train_loss: 0.01395 valid_loss: 0.13434 test_loss: 0.07596 \n",
      "[ 92/100] train_loss: 0.01333 valid_loss: 0.15728 test_loss: 0.07846 \n",
      "[ 93/100] train_loss: 0.01366 valid_loss: 0.12186 test_loss: 0.06613 \n",
      "[ 94/100] train_loss: 0.01331 valid_loss: 0.12242 test_loss: 0.07001 \n",
      "[ 95/100] train_loss: 0.01315 valid_loss: 0.12530 test_loss: 0.06591 \n",
      "[ 96/100] train_loss: 0.01266 valid_loss: 0.15213 test_loss: 0.07970 \n",
      "[ 97/100] train_loss: 0.01341 valid_loss: 0.11688 test_loss: 0.07092 \n",
      "[ 98/100] train_loss: 0.01250 valid_loss: 0.13383 test_loss: 0.07445 \n",
      "[ 99/100] train_loss: 0.01257 valid_loss: 0.12481 test_loss: 0.07205 \n",
      "[100/100] train_loss: 0.01229 valid_loss: 0.13365 test_loss: 0.06697 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "fn = '../data/refit/network_weights/REFIT_dishwasher_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e040173-5751-4b4b-9566-5f01716adb57",
   "metadata": {},
   "source": [
    "##### Washing-machine case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74515ab-9442-40f4-bb6a-264782bd3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['washing_machine']\n",
    "THRESHOLD = [30.]\n",
    "MIN_ON = [30.]\n",
    "MIN_OFF = [3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cccfbc7-7b3a-4c13-be3f-017d2f4a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses = [2,5,7,9,15,16,17,18,8]\n",
    "\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/washing_machine/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3537cf55-30c0-4091-8bda-8bc2e24d0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i], SEQ_LEN, BORDER, MAX_POWER, True) for i in range(7)]\n",
    "ds_house_total.append(Power(original_meter[7], ds_appliance[7], ds_status[7], SEQ_LEN, BORDER, MAX_POWER, False))\n",
    "ds_house_total.append(Power(original_meter[8], ds_appliance[8], ds_status[8], SEQ_LEN, BORDER, MAX_POWER, False))\n",
    "\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_total[0], \n",
    "                                                ds_house_total[1], \n",
    "                                                ds_house_total[2],\n",
    "                                                ds_house_total[3],\n",
    "                                                ds_house_total[4],\n",
    "                                                ds_house_total[5],\n",
    "                                                ds_house_total[6]])\n",
    "ds_valid       = torch.utils.data.ConcatDataset([ds_house_total[7]])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(dataset = ds_house_total[8], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_total[8], batch_size = 1, shuffle=False)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_total[7], batch_size = 1, shuffle=False)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e197fdf4-b3bd-4419-81bf-752687dcac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.18894 valid_loss: 0.05952 test_loss: 0.09366 \n",
      "Validation loss decreased (inf --> 0.059522).  Saving model ...\n",
      "[  2/100] train_loss: 0.10294 valid_loss: 0.09783 test_loss: 0.10452 \n",
      "[  3/100] train_loss: 0.09155 valid_loss: 0.10587 test_loss: 0.09763 \n",
      "[  4/100] train_loss: 0.08521 valid_loss: 0.07743 test_loss: 0.06716 \n",
      "[  5/100] train_loss: 0.07977 valid_loss: 0.05774 test_loss: 0.06596 \n",
      "Validation loss decreased (0.059522 --> 0.057741).  Saving model ...\n",
      "[  6/100] train_loss: 0.07846 valid_loss: 0.05603 test_loss: 0.06466 \n",
      "Validation loss decreased (0.057741 --> 0.056030).  Saving model ...\n",
      "[  7/100] train_loss: 0.07566 valid_loss: 0.04998 test_loss: 0.05833 \n",
      "Validation loss decreased (0.056030 --> 0.049977).  Saving model ...\n",
      "[  8/100] train_loss: 0.07505 valid_loss: 0.06249 test_loss: 0.06172 \n",
      "[  9/100] train_loss: 0.07510 valid_loss: 0.04661 test_loss: 0.05857 \n",
      "Validation loss decreased (0.049977 --> 0.046607).  Saving model ...\n",
      "[ 10/100] train_loss: 0.07461 valid_loss: 0.04130 test_loss: 0.05594 \n",
      "Validation loss decreased (0.046607 --> 0.041297).  Saving model ...\n",
      "[ 11/100] train_loss: 0.07325 valid_loss: 0.03480 test_loss: 0.05312 \n",
      "Validation loss decreased (0.041297 --> 0.034801).  Saving model ...\n",
      "[ 12/100] train_loss: 0.07109 valid_loss: 0.03971 test_loss: 0.05085 \n",
      "[ 13/100] train_loss: 0.06615 valid_loss: 0.03865 test_loss: 0.05484 \n",
      "[ 14/100] train_loss: 0.06692 valid_loss: 0.03168 test_loss: 0.05109 \n",
      "Validation loss decreased (0.034801 --> 0.031679).  Saving model ...\n",
      "[ 15/100] train_loss: 0.07307 valid_loss: 0.03527 test_loss: 0.05129 \n",
      "[ 16/100] train_loss: 0.07152 valid_loss: 0.03760 test_loss: 0.05090 \n",
      "[ 17/100] train_loss: 0.06758 valid_loss: 0.04880 test_loss: 0.05431 \n",
      "[ 18/100] train_loss: 0.06676 valid_loss: 0.04171 test_loss: 0.04963 \n",
      "[ 19/100] train_loss: 0.07123 valid_loss: 0.03905 test_loss: 0.05426 \n",
      "[ 20/100] train_loss: 0.06614 valid_loss: 0.03814 test_loss: 0.04730 \n",
      "[ 21/100] train_loss: 0.06843 valid_loss: 0.03196 test_loss: 0.04911 \n",
      "[ 22/100] train_loss: 0.07030 valid_loss: 0.03017 test_loss: 0.04474 \n",
      "Validation loss decreased (0.031679 --> 0.030168).  Saving model ...\n",
      "[ 23/100] train_loss: 0.06104 valid_loss: 0.02730 test_loss: 0.04590 \n",
      "Validation loss decreased (0.030168 --> 0.027301).  Saving model ...\n",
      "[ 24/100] train_loss: 0.06453 valid_loss: 0.04273 test_loss: 0.05295 \n",
      "[ 25/100] train_loss: 0.06101 valid_loss: 0.03029 test_loss: 0.04505 \n",
      "[ 26/100] train_loss: 0.06185 valid_loss: 0.02707 test_loss: 0.04544 \n",
      "Validation loss decreased (0.027301 --> 0.027075).  Saving model ...\n",
      "[ 27/100] train_loss: 0.06202 valid_loss: 0.03397 test_loss: 0.04696 \n",
      "[ 28/100] train_loss: 0.06416 valid_loss: 0.03429 test_loss: 0.04623 \n",
      "[ 29/100] train_loss: 0.06184 valid_loss: 0.03520 test_loss: 0.04583 \n",
      "[ 30/100] train_loss: 0.06015 valid_loss: 0.02598 test_loss: 0.04645 \n",
      "Validation loss decreased (0.027075 --> 0.025975).  Saving model ...\n",
      "[ 31/100] train_loss: 0.05809 valid_loss: 0.02511 test_loss: 0.04366 \n",
      "Validation loss decreased (0.025975 --> 0.025113).  Saving model ...\n",
      "[ 32/100] train_loss: 0.06184 valid_loss: 0.03224 test_loss: 0.04524 \n",
      "[ 33/100] train_loss: 0.06199 valid_loss: 0.03089 test_loss: 0.04246 \n",
      "[ 34/100] train_loss: 0.06304 valid_loss: 0.02805 test_loss: 0.04416 \n",
      "[ 35/100] train_loss: 0.06159 valid_loss: 0.02951 test_loss: 0.04405 \n",
      "[ 36/100] train_loss: 0.06016 valid_loss: 0.02748 test_loss: 0.04185 \n",
      "[ 37/100] train_loss: 0.06267 valid_loss: 0.03387 test_loss: 0.04396 \n",
      "[ 38/100] train_loss: 0.06311 valid_loss: 0.03118 test_loss: 0.04596 \n",
      "[ 39/100] train_loss: 0.05967 valid_loss: 0.02712 test_loss: 0.04460 \n",
      "[ 40/100] train_loss: 0.06304 valid_loss: 0.02638 test_loss: 0.04622 \n",
      "[ 41/100] train_loss: 0.05824 valid_loss: 0.03047 test_loss: 0.04313 \n",
      "[ 42/100] train_loss: 0.06282 valid_loss: 0.02727 test_loss: 0.04539 \n",
      "[ 43/100] train_loss: 0.05580 valid_loss: 0.03064 test_loss: 0.04505 \n",
      "[ 44/100] train_loss: 0.05599 valid_loss: 0.03449 test_loss: 0.04858 \n",
      "[ 45/100] train_loss: 0.06144 valid_loss: 0.03093 test_loss: 0.04809 \n",
      "[ 46/100] train_loss: 0.05613 valid_loss: 0.03114 test_loss: 0.04352 \n",
      "[ 47/100] train_loss: 0.05331 valid_loss: 0.02695 test_loss: 0.04698 \n",
      "[ 48/100] train_loss: 0.05774 valid_loss: 0.02549 test_loss: 0.04320 \n",
      "[ 49/100] train_loss: 0.05709 valid_loss: 0.02703 test_loss: 0.04184 \n",
      "[ 50/100] train_loss: 0.05582 valid_loss: 0.02971 test_loss: 0.04454 \n",
      "[ 51/100] train_loss: 0.05711 valid_loss: 0.03060 test_loss: 0.04530 \n",
      "[ 52/100] train_loss: 0.05665 valid_loss: 0.02985 test_loss: 0.04263 \n",
      "[ 53/100] train_loss: 0.05411 valid_loss: 0.02689 test_loss: 0.04348 \n",
      "[ 54/100] train_loss: 0.05679 valid_loss: 0.02992 test_loss: 0.04560 \n",
      "[ 55/100] train_loss: 0.05751 valid_loss: 0.02626 test_loss: 0.04297 \n",
      "[ 56/100] train_loss: 0.05530 valid_loss: 0.02168 test_loss: 0.04080 \n",
      "Validation loss decreased (0.025113 --> 0.021676).  Saving model ...\n",
      "[ 57/100] train_loss: 0.05402 valid_loss: 0.02676 test_loss: 0.04285 \n",
      "[ 58/100] train_loss: 0.05308 valid_loss: 0.03029 test_loss: 0.04367 \n",
      "[ 59/100] train_loss: 0.05602 valid_loss: 0.02100 test_loss: 0.04589 \n",
      "Validation loss decreased (0.021676 --> 0.020998).  Saving model ...\n",
      "[ 60/100] train_loss: 0.05691 valid_loss: 0.02495 test_loss: 0.04278 \n",
      "[ 61/100] train_loss: 0.05433 valid_loss: 0.02891 test_loss: 0.04135 \n",
      "[ 62/100] train_loss: 0.05779 valid_loss: 0.02380 test_loss: 0.04148 \n",
      "[ 63/100] train_loss: 0.05766 valid_loss: 0.03089 test_loss: 0.04301 \n",
      "[ 64/100] train_loss: 0.05708 valid_loss: 0.02843 test_loss: 0.04197 \n",
      "[ 65/100] train_loss: 0.05428 valid_loss: 0.02297 test_loss: 0.04096 \n",
      "[ 66/100] train_loss: 0.05280 valid_loss: 0.02304 test_loss: 0.04156 \n",
      "[ 67/100] train_loss: 0.05296 valid_loss: 0.03811 test_loss: 0.04770 \n",
      "[ 68/100] train_loss: 0.05293 valid_loss: 0.02575 test_loss: 0.04242 \n",
      "[ 69/100] train_loss: 0.05086 valid_loss: 0.02571 test_loss: 0.04407 \n",
      "[ 70/100] train_loss: 0.05082 valid_loss: 0.02168 test_loss: 0.04627 \n",
      "[ 71/100] train_loss: 0.05295 valid_loss: 0.03098 test_loss: 0.04336 \n",
      "[ 72/100] train_loss: 0.05490 valid_loss: 0.03478 test_loss: 0.04659 \n",
      "[ 73/100] train_loss: 0.05165 valid_loss: 0.02587 test_loss: 0.04558 \n",
      "[ 74/100] train_loss: 0.05352 valid_loss: 0.01984 test_loss: 0.03970 \n",
      "Validation loss decreased (0.020998 --> 0.019837).  Saving model ...\n",
      "[ 75/100] train_loss: 0.04519 valid_loss: 0.02120 test_loss: 0.03955 \n",
      "[ 76/100] train_loss: 0.05158 valid_loss: 0.02082 test_loss: 0.04027 \n",
      "[ 77/100] train_loss: 0.05219 valid_loss: 0.02207 test_loss: 0.04199 \n",
      "[ 78/100] train_loss: 0.05322 valid_loss: 0.02188 test_loss: 0.04133 \n",
      "[ 79/100] train_loss: 0.05509 valid_loss: 0.02425 test_loss: 0.04528 \n",
      "[ 80/100] train_loss: 0.04936 valid_loss: 0.02254 test_loss: 0.04367 \n",
      "[ 81/100] train_loss: 0.05010 valid_loss: 0.02215 test_loss: 0.04285 \n",
      "[ 82/100] train_loss: 0.05081 valid_loss: 0.02206 test_loss: 0.04014 \n",
      "[ 83/100] train_loss: 0.05236 valid_loss: 0.02638 test_loss: 0.04448 \n",
      "[ 84/100] train_loss: 0.04772 valid_loss: 0.01842 test_loss: 0.04320 \n",
      "Validation loss decreased (0.019837 --> 0.018425).  Saving model ...\n",
      "[ 85/100] train_loss: 0.05047 valid_loss: 0.02674 test_loss: 0.04175 \n",
      "[ 86/100] train_loss: 0.05217 valid_loss: 0.02501 test_loss: 0.04113 \n",
      "[ 87/100] train_loss: 0.05012 valid_loss: 0.03352 test_loss: 0.04416 \n",
      "[ 88/100] train_loss: 0.04819 valid_loss: 0.01783 test_loss: 0.04050 \n",
      "Validation loss decreased (0.018425 --> 0.017828).  Saving model ...\n",
      "[ 89/100] train_loss: 0.04943 valid_loss: 0.02188 test_loss: 0.04427 \n",
      "[ 90/100] train_loss: 0.04877 valid_loss: 0.02279 test_loss: 0.04600 \n",
      "[ 91/100] train_loss: 0.04819 valid_loss: 0.02231 test_loss: 0.04441 \n",
      "[ 92/100] train_loss: 0.05056 valid_loss: 0.03138 test_loss: 0.04154 \n",
      "[ 93/100] train_loss: 0.05075 valid_loss: 0.02211 test_loss: 0.04300 \n",
      "[ 94/100] train_loss: 0.04976 valid_loss: 0.02606 test_loss: 0.04396 \n",
      "[ 95/100] train_loss: 0.04906 valid_loss: 0.02386 test_loss: 0.04105 \n",
      "[ 96/100] train_loss: 0.04785 valid_loss: 0.02080 test_loss: 0.04509 \n",
      "[ 97/100] train_loss: 0.05123 valid_loss: 0.02428 test_loss: 0.04415 \n",
      "[ 98/100] train_loss: 0.04901 valid_loss: 0.02845 test_loss: 0.04346 \n",
      "[ 99/100] train_loss: 0.05073 valid_loss: 0.02818 test_loss: 0.04412 \n",
      "[100/100] train_loss: 0.04960 valid_loss: 0.02138 test_loss: 0.04700 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "fn = '../data/refit/network_weights/REFIT_washingmachine_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9434991-37cf-4d74-8257-3858a7c7699c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
