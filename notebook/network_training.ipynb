{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6c3d26-334e-40e2-b5e7-b1a5fb4dc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f707559-2e56-4496-aa1c-ea01e66da91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in /root/.local/lib/python3.9/site-packages (3.7.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /root/.local/lib/python3.9/site-packages (from tables) (2.8.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from tables) (1.23.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tables) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->tables) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ba3b5da-a543-4e2c-9fd9-cc5b95fa40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 3., 30.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0b83-ea9e-4f00-adb4-a582e02fd6cc",
   "metadata": {},
   "source": [
    "# UKDale training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d621239f-ab4f-4281-8ea1-7d13667ea723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "houses=[1,2,5]\n",
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/ukdale/feather_files/UKDALE_%d_train.feather' %(i))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "  \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38ba56dc-d29b-4258-8eeb-dccf7cc894ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(3)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(3)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(3)]\n",
    "\n",
    "ds_train = torch.utils.data.ConcatDataset([ds_house_train[0], ds_house_train[2]])\n",
    "ds_valid = torch.utils.data.ConcatDataset([ds_house_valid[0], ds_house_valid[2]])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_test = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_total[1], batch_size = 1, shuffle=False)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in [0,2]]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=True) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fe51444-765c-47a9-82a5-ea7ecd25dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/500] train_loss: 0.58881 valid_loss: 0.51376 test_loss: 0.49512 \n",
      "Validation loss decreased (inf --> 0.513759).  Saving model ...\n",
      "[  2/500] train_loss: 0.46391 valid_loss: 0.41034 test_loss: 0.36804 \n",
      "Validation loss decreased (0.513759 --> 0.410338).  Saving model ...\n",
      "[  3/500] train_loss: 0.38047 valid_loss: 0.35669 test_loss: 0.31420 \n",
      "Validation loss decreased (0.410338 --> 0.356688).  Saving model ...\n",
      "[  4/500] train_loss: 0.32846 valid_loss: 0.32315 test_loss: 0.27957 \n",
      "Validation loss decreased (0.356688 --> 0.323147).  Saving model ...\n",
      "[  5/500] train_loss: 0.29743 valid_loss: 0.29745 test_loss: 0.25475 \n",
      "Validation loss decreased (0.323147 --> 0.297453).  Saving model ...\n",
      "[  6/500] train_loss: 0.27813 valid_loss: 0.27619 test_loss: 0.23219 \n",
      "Validation loss decreased (0.297453 --> 0.276195).  Saving model ...\n",
      "[  7/500] train_loss: 0.25326 valid_loss: 0.25945 test_loss: 0.21463 \n",
      "Validation loss decreased (0.276195 --> 0.259449).  Saving model ...\n",
      "[  8/500] train_loss: 0.23733 valid_loss: 0.24252 test_loss: 0.19927 \n",
      "Validation loss decreased (0.259449 --> 0.242521).  Saving model ...\n",
      "[  9/500] train_loss: 0.22921 valid_loss: 0.22974 test_loss: 0.18601 \n",
      "Validation loss decreased (0.242521 --> 0.229738).  Saving model ...\n",
      "[ 10/500] train_loss: 0.20976 valid_loss: 0.22031 test_loss: 0.17392 \n",
      "Validation loss decreased (0.229738 --> 0.220311).  Saving model ...\n",
      "[ 11/500] train_loss: 0.20232 valid_loss: 0.20998 test_loss: 0.16471 \n",
      "Validation loss decreased (0.220311 --> 0.209985).  Saving model ...\n",
      "[ 12/500] train_loss: 0.19720 valid_loss: 0.20298 test_loss: 0.15810 \n",
      "Validation loss decreased (0.209985 --> 0.202979).  Saving model ...\n",
      "[ 13/500] train_loss: 0.18869 valid_loss: 0.19701 test_loss: 0.15314 \n",
      "Validation loss decreased (0.202979 --> 0.197006).  Saving model ...\n",
      "[ 14/500] train_loss: 0.18115 valid_loss: 0.19221 test_loss: 0.14516 \n",
      "Validation loss decreased (0.197006 --> 0.192209).  Saving model ...\n",
      "[ 15/500] train_loss: 0.17343 valid_loss: 0.18727 test_loss: 0.14115 \n",
      "Validation loss decreased (0.192209 --> 0.187271).  Saving model ...\n",
      "[ 16/500] train_loss: 0.17219 valid_loss: 0.18394 test_loss: 0.13829 \n",
      "Validation loss decreased (0.187271 --> 0.183938).  Saving model ...\n",
      "[ 17/500] train_loss: 0.16855 valid_loss: 0.17923 test_loss: 0.13737 \n",
      "Validation loss decreased (0.183938 --> 0.179229).  Saving model ...\n",
      "[ 18/500] train_loss: 0.16646 valid_loss: 0.17535 test_loss: 0.13227 \n",
      "Validation loss decreased (0.179229 --> 0.175346).  Saving model ...\n",
      "[ 19/500] train_loss: 0.16408 valid_loss: 0.17406 test_loss: 0.12960 \n",
      "Validation loss decreased (0.175346 --> 0.174064).  Saving model ...\n",
      "[ 20/500] train_loss: 0.15773 valid_loss: 0.17057 test_loss: 0.12521 \n",
      "Validation loss decreased (0.174064 --> 0.170570).  Saving model ...\n",
      "[ 21/500] train_loss: 0.15848 valid_loss: 0.16918 test_loss: 0.12593 \n",
      "Validation loss decreased (0.170570 --> 0.169177).  Saving model ...\n",
      "[ 22/500] train_loss: 0.15697 valid_loss: 0.16786 test_loss: 0.12235 \n",
      "Validation loss decreased (0.169177 --> 0.167857).  Saving model ...\n",
      "[ 23/500] train_loss: 0.15539 valid_loss: 0.16456 test_loss: 0.12091 \n",
      "Validation loss decreased (0.167857 --> 0.164559).  Saving model ...\n",
      "[ 24/500] train_loss: 0.14991 valid_loss: 0.16640 test_loss: 0.12320 \n",
      "[ 25/500] train_loss: 0.15028 valid_loss: 0.16142 test_loss: 0.11984 \n",
      "Validation loss decreased (0.164559 --> 0.161417).  Saving model ...\n",
      "[ 26/500] train_loss: 0.15058 valid_loss: 0.15981 test_loss: 0.11766 \n",
      "Validation loss decreased (0.161417 --> 0.159805).  Saving model ...\n",
      "[ 27/500] train_loss: 0.14550 valid_loss: 0.15795 test_loss: 0.11471 \n",
      "Validation loss decreased (0.159805 --> 0.157953).  Saving model ...\n",
      "[ 28/500] train_loss: 0.14350 valid_loss: 0.15722 test_loss: 0.11326 \n",
      "Validation loss decreased (0.157953 --> 0.157216).  Saving model ...\n",
      "[ 29/500] train_loss: 0.14569 valid_loss: 0.15768 test_loss: 0.11541 \n",
      "[ 30/500] train_loss: 0.14681 valid_loss: 0.15636 test_loss: 0.11359 \n",
      "Validation loss decreased (0.157216 --> 0.156358).  Saving model ...\n",
      "[ 31/500] train_loss: 0.14441 valid_loss: 0.15811 test_loss: 0.11722 \n",
      "[ 32/500] train_loss: 0.14027 valid_loss: 0.15189 test_loss: 0.11136 \n",
      "Validation loss decreased (0.156358 --> 0.151891).  Saving model ...\n",
      "[ 33/500] train_loss: 0.13795 valid_loss: 0.15264 test_loss: 0.10944 \n",
      "[ 34/500] train_loss: 0.14184 valid_loss: 0.15426 test_loss: 0.11334 \n",
      "[ 35/500] train_loss: 0.14195 valid_loss: 0.15437 test_loss: 0.11158 \n",
      "[ 36/500] train_loss: 0.13928 valid_loss: 0.14976 test_loss: 0.10898 \n",
      "Validation loss decreased (0.151891 --> 0.149763).  Saving model ...\n",
      "[ 37/500] train_loss: 0.13668 valid_loss: 0.15344 test_loss: 0.11264 \n",
      "[ 38/500] train_loss: 0.13503 valid_loss: 0.14888 test_loss: 0.10700 \n",
      "Validation loss decreased (0.149763 --> 0.148881).  Saving model ...\n",
      "[ 39/500] train_loss: 0.13456 valid_loss: 0.14765 test_loss: 0.10834 \n",
      "Validation loss decreased (0.148881 --> 0.147645).  Saving model ...\n",
      "[ 40/500] train_loss: 0.13266 valid_loss: 0.14578 test_loss: 0.10676 \n",
      "Validation loss decreased (0.147645 --> 0.145775).  Saving model ...\n",
      "[ 41/500] train_loss: 0.13367 valid_loss: 0.14690 test_loss: 0.10746 \n",
      "[ 42/500] train_loss: 0.13253 valid_loss: 0.14707 test_loss: 0.10902 \n",
      "[ 43/500] train_loss: 0.13322 valid_loss: 0.14453 test_loss: 0.10622 \n",
      "Validation loss decreased (0.145775 --> 0.144534).  Saving model ...\n",
      "[ 44/500] train_loss: 0.13247 valid_loss: 0.14554 test_loss: 0.10739 \n",
      "[ 45/500] train_loss: 0.12866 valid_loss: 0.14482 test_loss: 0.10617 \n",
      "[ 46/500] train_loss: 0.12766 valid_loss: 0.14363 test_loss: 0.10583 \n",
      "Validation loss decreased (0.144534 --> 0.143628).  Saving model ...\n",
      "[ 47/500] train_loss: 0.13130 valid_loss: 0.14311 test_loss: 0.10291 \n",
      "Validation loss decreased (0.143628 --> 0.143111).  Saving model ...\n",
      "[ 48/500] train_loss: 0.12807 valid_loss: 0.14089 test_loss: 0.10291 \n",
      "Validation loss decreased (0.143111 --> 0.140885).  Saving model ...\n",
      "[ 49/500] train_loss: 0.12990 valid_loss: 0.14412 test_loss: 0.10581 \n",
      "[ 50/500] train_loss: 0.12359 valid_loss: 0.14246 test_loss: 0.10508 \n",
      "[ 51/500] train_loss: 0.12792 valid_loss: 0.13887 test_loss: 0.10260 \n",
      "Validation loss decreased (0.140885 --> 0.138874).  Saving model ...\n",
      "[ 52/500] train_loss: 0.12553 valid_loss: 0.14097 test_loss: 0.10378 \n",
      "[ 53/500] train_loss: 0.12682 valid_loss: 0.13962 test_loss: 0.10358 \n",
      "[ 54/500] train_loss: 0.12362 valid_loss: 0.13752 test_loss: 0.10137 \n",
      "Validation loss decreased (0.138874 --> 0.137519).  Saving model ...\n",
      "[ 55/500] train_loss: 0.12391 valid_loss: 0.13941 test_loss: 0.10454 \n",
      "[ 56/500] train_loss: 0.12766 valid_loss: 0.13785 test_loss: 0.10367 \n",
      "[ 57/500] train_loss: 0.12262 valid_loss: 0.13529 test_loss: 0.10032 \n",
      "Validation loss decreased (0.137519 --> 0.135287).  Saving model ...\n",
      "[ 58/500] train_loss: 0.12604 valid_loss: 0.13599 test_loss: 0.10260 \n",
      "[ 59/500] train_loss: 0.12388 valid_loss: 0.13636 test_loss: 0.10118 \n",
      "[ 60/500] train_loss: 0.12318 valid_loss: 0.13524 test_loss: 0.10214 \n",
      "Validation loss decreased (0.135287 --> 0.135243).  Saving model ...\n",
      "[ 61/500] train_loss: 0.12323 valid_loss: 0.13646 test_loss: 0.10439 \n",
      "[ 62/500] train_loss: 0.11863 valid_loss: 0.13496 test_loss: 0.10195 \n",
      "Validation loss decreased (0.135243 --> 0.134963).  Saving model ...\n",
      "[ 63/500] train_loss: 0.12386 valid_loss: 0.13719 test_loss: 0.10366 \n",
      "[ 64/500] train_loss: 0.12106 valid_loss: 0.13551 test_loss: 0.10177 \n",
      "[ 65/500] train_loss: 0.11643 valid_loss: 0.13425 test_loss: 0.10122 \n",
      "Validation loss decreased (0.134963 --> 0.134255).  Saving model ...\n",
      "[ 66/500] train_loss: 0.12225 valid_loss: 0.13465 test_loss: 0.10124 \n",
      "[ 67/500] train_loss: 0.12134 valid_loss: 0.13234 test_loss: 0.09958 \n",
      "Validation loss decreased (0.134255 --> 0.132341).  Saving model ...\n",
      "[ 68/500] train_loss: 0.11750 valid_loss: 0.13179 test_loss: 0.09935 \n",
      "Validation loss decreased (0.132341 --> 0.131787).  Saving model ...\n",
      "[ 69/500] train_loss: 0.11831 valid_loss: 0.13317 test_loss: 0.09873 \n",
      "[ 70/500] train_loss: 0.11549 valid_loss: 0.13174 test_loss: 0.09912 \n",
      "Validation loss decreased (0.131787 --> 0.131736).  Saving model ...\n",
      "[ 71/500] train_loss: 0.11508 valid_loss: 0.13116 test_loss: 0.09823 \n",
      "Validation loss decreased (0.131736 --> 0.131164).  Saving model ...\n",
      "[ 72/500] train_loss: 0.11504 valid_loss: 0.13234 test_loss: 0.10226 \n",
      "[ 73/500] train_loss: 0.11889 valid_loss: 0.13286 test_loss: 0.10167 \n",
      "[ 74/500] train_loss: 0.11744 valid_loss: 0.13114 test_loss: 0.09955 \n",
      "Validation loss decreased (0.131164 --> 0.131140).  Saving model ...\n",
      "[ 75/500] train_loss: 0.12074 valid_loss: 0.12955 test_loss: 0.09762 \n",
      "Validation loss decreased (0.131140 --> 0.129546).  Saving model ...\n",
      "[ 76/500] train_loss: 0.11808 valid_loss: 0.13075 test_loss: 0.09884 \n",
      "[ 77/500] train_loss: 0.11822 valid_loss: 0.13107 test_loss: 0.09971 \n",
      "[ 78/500] train_loss: 0.11673 valid_loss: 0.13010 test_loss: 0.10002 \n",
      "[ 79/500] train_loss: 0.11444 valid_loss: 0.12789 test_loss: 0.09854 \n",
      "Validation loss decreased (0.129546 --> 0.127889).  Saving model ...\n",
      "[ 80/500] train_loss: 0.11814 valid_loss: 0.12968 test_loss: 0.09991 \n",
      "[ 81/500] train_loss: 0.11674 valid_loss: 0.12960 test_loss: 0.09992 \n",
      "[ 82/500] train_loss: 0.11577 valid_loss: 0.12752 test_loss: 0.09743 \n",
      "Validation loss decreased (0.127889 --> 0.127519).  Saving model ...\n",
      "[ 83/500] train_loss: 0.11572 valid_loss: 0.12746 test_loss: 0.09663 \n",
      "Validation loss decreased (0.127519 --> 0.127464).  Saving model ...\n",
      "[ 84/500] train_loss: 0.11672 valid_loss: 0.12774 test_loss: 0.09670 \n",
      "[ 85/500] train_loss: 0.11303 valid_loss: 0.12725 test_loss: 0.09759 \n",
      "Validation loss decreased (0.127464 --> 0.127247).  Saving model ...\n",
      "[ 86/500] train_loss: 0.11372 valid_loss: 0.12897 test_loss: 0.09861 \n",
      "[ 87/500] train_loss: 0.11158 valid_loss: 0.13045 test_loss: 0.10169 \n",
      "[ 88/500] train_loss: 0.11272 valid_loss: 0.12777 test_loss: 0.09659 \n",
      "[ 89/500] train_loss: 0.11183 valid_loss: 0.12739 test_loss: 0.09774 \n",
      "[ 90/500] train_loss: 0.11099 valid_loss: 0.12830 test_loss: 0.09843 \n",
      "[ 91/500] train_loss: 0.11499 valid_loss: 0.12411 test_loss: 0.09449 \n",
      "Validation loss decreased (0.127247 --> 0.124111).  Saving model ...\n",
      "[ 92/500] train_loss: 0.11055 valid_loss: 0.12875 test_loss: 0.10096 \n",
      "[ 93/500] train_loss: 0.10975 valid_loss: 0.12436 test_loss: 0.09498 \n",
      "[ 94/500] train_loss: 0.10727 valid_loss: 0.12449 test_loss: 0.09571 \n",
      "[ 95/500] train_loss: 0.10819 valid_loss: 0.12396 test_loss: 0.09510 \n",
      "Validation loss decreased (0.124111 --> 0.123964).  Saving model ...\n",
      "[ 96/500] train_loss: 0.11222 valid_loss: 0.12407 test_loss: 0.09456 \n",
      "[ 97/500] train_loss: 0.10833 valid_loss: 0.12445 test_loss: 0.09599 \n",
      "[ 98/500] train_loss: 0.11122 valid_loss: 0.12511 test_loss: 0.09666 \n",
      "[ 99/500] train_loss: 0.10767 valid_loss: 0.12270 test_loss: 0.09512 \n",
      "Validation loss decreased (0.123964 --> 0.122698).  Saving model ...\n",
      "[100/500] train_loss: 0.10633 valid_loss: 0.12234 test_loss: 0.09501 \n",
      "Validation loss decreased (0.122698 --> 0.122344).  Saving model ...\n",
      "[101/500] train_loss: 0.11089 valid_loss: 0.12430 test_loss: 0.09552 \n",
      "[102/500] train_loss: 0.10668 valid_loss: 0.12389 test_loss: 0.09385 \n",
      "[103/500] train_loss: 0.10518 valid_loss: 0.12398 test_loss: 0.09492 \n",
      "[104/500] train_loss: 0.10909 valid_loss: 0.12155 test_loss: 0.09269 \n",
      "Validation loss decreased (0.122344 --> 0.121550).  Saving model ...\n",
      "[105/500] train_loss: 0.10682 valid_loss: 0.12145 test_loss: 0.09201 \n",
      "Validation loss decreased (0.121550 --> 0.121454).  Saving model ...\n",
      "[106/500] train_loss: 0.10732 valid_loss: 0.12284 test_loss: 0.09326 \n",
      "[107/500] train_loss: 0.10823 valid_loss: 0.12336 test_loss: 0.09622 \n",
      "[108/500] train_loss: 0.10603 valid_loss: 0.12046 test_loss: 0.09258 \n",
      "Validation loss decreased (0.121454 --> 0.120464).  Saving model ...\n",
      "[109/500] train_loss: 0.10723 valid_loss: 0.12123 test_loss: 0.09388 \n",
      "[110/500] train_loss: 0.10910 valid_loss: 0.12156 test_loss: 0.09279 \n",
      "[111/500] train_loss: 0.10586 valid_loss: 0.12282 test_loss: 0.09381 \n",
      "[112/500] train_loss: 0.10382 valid_loss: 0.11973 test_loss: 0.09050 \n",
      "Validation loss decreased (0.120464 --> 0.119727).  Saving model ...\n",
      "[113/500] train_loss: 0.10485 valid_loss: 0.12012 test_loss: 0.09288 \n",
      "[114/500] train_loss: 0.10890 valid_loss: 0.12279 test_loss: 0.09505 \n",
      "[115/500] train_loss: 0.10574 valid_loss: 0.12143 test_loss: 0.09408 \n",
      "[116/500] train_loss: 0.10707 valid_loss: 0.12010 test_loss: 0.09033 \n",
      "[117/500] train_loss: 0.10498 valid_loss: 0.12000 test_loss: 0.09179 \n",
      "[118/500] train_loss: 0.10128 valid_loss: 0.11928 test_loss: 0.08988 \n",
      "Validation loss decreased (0.119727 --> 0.119282).  Saving model ...\n",
      "[119/500] train_loss: 0.10546 valid_loss: 0.11757 test_loss: 0.09148 \n",
      "Validation loss decreased (0.119282 --> 0.117571).  Saving model ...\n",
      "[120/500] train_loss: 0.10303 valid_loss: 0.11919 test_loss: 0.09193 \n",
      "[121/500] train_loss: 0.10453 valid_loss: 0.11749 test_loss: 0.09072 \n",
      "Validation loss decreased (0.117571 --> 0.117491).  Saving model ...\n",
      "[122/500] train_loss: 0.10287 valid_loss: 0.11785 test_loss: 0.08802 \n",
      "[123/500] train_loss: 0.10290 valid_loss: 0.12141 test_loss: 0.09400 \n",
      "[124/500] train_loss: 0.10396 valid_loss: 0.11857 test_loss: 0.09066 \n",
      "[125/500] train_loss: 0.10371 valid_loss: 0.11900 test_loss: 0.09339 \n",
      "[126/500] train_loss: 0.10460 valid_loss: 0.11767 test_loss: 0.09143 \n",
      "[127/500] train_loss: 0.10276 valid_loss: 0.11683 test_loss: 0.08907 \n",
      "Validation loss decreased (0.117491 --> 0.116825).  Saving model ...\n",
      "[128/500] train_loss: 0.10231 valid_loss: 0.11892 test_loss: 0.09243 \n",
      "[129/500] train_loss: 0.10268 valid_loss: 0.11728 test_loss: 0.09013 \n",
      "[130/500] train_loss: 0.10533 valid_loss: 0.11827 test_loss: 0.09093 \n",
      "[131/500] train_loss: 0.10318 valid_loss: 0.11744 test_loss: 0.09016 \n",
      "[132/500] train_loss: 0.10763 valid_loss: 0.11750 test_loss: 0.08982 \n",
      "[133/500] train_loss: 0.10286 valid_loss: 0.11598 test_loss: 0.08759 \n",
      "Validation loss decreased (0.116825 --> 0.115984).  Saving model ...\n",
      "[134/500] train_loss: 0.10485 valid_loss: 0.11632 test_loss: 0.09013 \n",
      "[135/500] train_loss: 0.09903 valid_loss: 0.11569 test_loss: 0.08925 \n",
      "Validation loss decreased (0.115984 --> 0.115691).  Saving model ...\n",
      "[136/500] train_loss: 0.10063 valid_loss: 0.11629 test_loss: 0.08891 \n",
      "[137/500] train_loss: 0.10081 valid_loss: 0.11591 test_loss: 0.08924 \n",
      "[138/500] train_loss: 0.10182 valid_loss: 0.11675 test_loss: 0.08779 \n",
      "[139/500] train_loss: 0.10106 valid_loss: 0.11588 test_loss: 0.08932 \n",
      "[140/500] train_loss: 0.10151 valid_loss: 0.11689 test_loss: 0.09137 \n",
      "[141/500] train_loss: 0.09947 valid_loss: 0.11687 test_loss: 0.08993 \n",
      "[142/500] train_loss: 0.10029 valid_loss: 0.11505 test_loss: 0.08906 \n",
      "Validation loss decreased (0.115691 --> 0.115052).  Saving model ...\n",
      "[143/500] train_loss: 0.10096 valid_loss: 0.11525 test_loss: 0.08884 \n",
      "[144/500] train_loss: 0.09948 valid_loss: 0.11596 test_loss: 0.09027 \n",
      "[145/500] train_loss: 0.10146 valid_loss: 0.11428 test_loss: 0.08775 \n",
      "Validation loss decreased (0.115052 --> 0.114283).  Saving model ...\n",
      "[146/500] train_loss: 0.10075 valid_loss: 0.11472 test_loss: 0.08889 \n",
      "[147/500] train_loss: 0.09967 valid_loss: 0.11502 test_loss: 0.08644 \n",
      "[148/500] train_loss: 0.09900 valid_loss: 0.11402 test_loss: 0.08882 \n",
      "Validation loss decreased (0.114283 --> 0.114019).  Saving model ...\n",
      "[149/500] train_loss: 0.09952 valid_loss: 0.11445 test_loss: 0.08881 \n",
      "[150/500] train_loss: 0.09836 valid_loss: 0.11324 test_loss: 0.08588 \n",
      "Validation loss decreased (0.114019 --> 0.113242).  Saving model ...\n",
      "[151/500] train_loss: 0.09921 valid_loss: 0.11311 test_loss: 0.08819 \n",
      "Validation loss decreased (0.113242 --> 0.113108).  Saving model ...\n",
      "[152/500] train_loss: 0.09902 valid_loss: 0.11352 test_loss: 0.08634 \n",
      "[153/500] train_loss: 0.10283 valid_loss: 0.11253 test_loss: 0.08898 \n",
      "Validation loss decreased (0.113108 --> 0.112527).  Saving model ...\n",
      "[154/500] train_loss: 0.10003 valid_loss: 0.11342 test_loss: 0.08890 \n",
      "[155/500] train_loss: 0.09920 valid_loss: 0.11368 test_loss: 0.08901 \n",
      "[156/500] train_loss: 0.09992 valid_loss: 0.11140 test_loss: 0.08449 \n",
      "Validation loss decreased (0.112527 --> 0.111397).  Saving model ...\n",
      "[157/500] train_loss: 0.09784 valid_loss: 0.11314 test_loss: 0.08588 \n",
      "[158/500] train_loss: 0.09647 valid_loss: 0.11281 test_loss: 0.08457 \n",
      "[159/500] train_loss: 0.10003 valid_loss: 0.11236 test_loss: 0.08671 \n",
      "[160/500] train_loss: 0.09606 valid_loss: 0.11249 test_loss: 0.08607 \n",
      "[161/500] train_loss: 0.09780 valid_loss: 0.11182 test_loss: 0.08488 \n",
      "[162/500] train_loss: 0.09796 valid_loss: 0.11122 test_loss: 0.08620 \n",
      "Validation loss decreased (0.111397 --> 0.111221).  Saving model ...\n",
      "[163/500] train_loss: 0.09523 valid_loss: 0.11293 test_loss: 0.08829 \n",
      "[164/500] train_loss: 0.09768 valid_loss: 0.11195 test_loss: 0.08538 \n",
      "[165/500] train_loss: 0.09728 valid_loss: 0.11197 test_loss: 0.08544 \n",
      "[166/500] train_loss: 0.09843 valid_loss: 0.11213 test_loss: 0.08526 \n",
      "[167/500] train_loss: 0.09372 valid_loss: 0.11190 test_loss: 0.08526 \n",
      "[168/500] train_loss: 0.09539 valid_loss: 0.11178 test_loss: 0.08693 \n",
      "[169/500] train_loss: 0.09767 valid_loss: 0.11340 test_loss: 0.08903 \n",
      "[170/500] train_loss: 0.09613 valid_loss: 0.11336 test_loss: 0.08650 \n",
      "[171/500] train_loss: 0.09675 valid_loss: 0.11231 test_loss: 0.08658 \n",
      "[172/500] train_loss: 0.09929 valid_loss: 0.11159 test_loss: 0.08648 \n",
      "[173/500] train_loss: 0.09427 valid_loss: 0.11312 test_loss: 0.08901 \n",
      "[174/500] train_loss: 0.09677 valid_loss: 0.11072 test_loss: 0.08424 \n",
      "Validation loss decreased (0.111221 --> 0.110719).  Saving model ...\n",
      "[175/500] train_loss: 0.09406 valid_loss: 0.11148 test_loss: 0.08651 \n",
      "[176/500] train_loss: 0.09643 valid_loss: 0.11018 test_loss: 0.08509 \n",
      "Validation loss decreased (0.110719 --> 0.110175).  Saving model ...\n",
      "[177/500] train_loss: 0.09441 valid_loss: 0.11121 test_loss: 0.08602 \n",
      "[178/500] train_loss: 0.09411 valid_loss: 0.11138 test_loss: 0.08482 \n",
      "[179/500] train_loss: 0.09561 valid_loss: 0.11068 test_loss: 0.08568 \n",
      "[180/500] train_loss: 0.09743 valid_loss: 0.11136 test_loss: 0.08831 \n",
      "[181/500] train_loss: 0.09539 valid_loss: 0.10989 test_loss: 0.08343 \n",
      "Validation loss decreased (0.110175 --> 0.109890).  Saving model ...\n",
      "[182/500] train_loss: 0.09347 valid_loss: 0.11129 test_loss: 0.08629 \n",
      "[183/500] train_loss: 0.09266 valid_loss: 0.10822 test_loss: 0.08390 \n",
      "Validation loss decreased (0.109890 --> 0.108217).  Saving model ...\n",
      "[184/500] train_loss: 0.09409 valid_loss: 0.11024 test_loss: 0.08390 \n",
      "[185/500] train_loss: 0.09606 valid_loss: 0.10914 test_loss: 0.08517 \n",
      "[186/500] train_loss: 0.09171 valid_loss: 0.11093 test_loss: 0.08952 \n",
      "[187/500] train_loss: 0.09022 valid_loss: 0.10905 test_loss: 0.08506 \n",
      "[188/500] train_loss: 0.09488 valid_loss: 0.10953 test_loss: 0.08390 \n",
      "[189/500] train_loss: 0.09586 valid_loss: 0.10932 test_loss: 0.08598 \n",
      "[190/500] train_loss: 0.09334 valid_loss: 0.10839 test_loss: 0.08396 \n",
      "[191/500] train_loss: 0.09473 valid_loss: 0.10896 test_loss: 0.08407 \n",
      "[192/500] train_loss: 0.09224 valid_loss: 0.10847 test_loss: 0.08411 \n",
      "[193/500] train_loss: 0.09679 valid_loss: 0.11025 test_loss: 0.08492 \n",
      "[194/500] train_loss: 0.09172 valid_loss: 0.11080 test_loss: 0.08589 \n",
      "[195/500] train_loss: 0.09342 valid_loss: 0.10976 test_loss: 0.08489 \n",
      "[196/500] train_loss: 0.09448 valid_loss: 0.10823 test_loss: 0.08389 \n",
      "[197/500] train_loss: 0.09743 valid_loss: 0.10864 test_loss: 0.08466 \n",
      "[198/500] train_loss: 0.09272 valid_loss: 0.10846 test_loss: 0.08557 \n",
      "[199/500] train_loss: 0.09041 valid_loss: 0.10844 test_loss: 0.08576 \n",
      "[200/500] train_loss: 0.09287 valid_loss: 0.10801 test_loss: 0.08439 \n",
      "Validation loss decreased (0.108217 --> 0.108008).  Saving model ...\n",
      "[201/500] train_loss: 0.09190 valid_loss: 0.10750 test_loss: 0.08432 \n",
      "Validation loss decreased (0.108008 --> 0.107503).  Saving model ...\n",
      "[202/500] train_loss: 0.09295 valid_loss: 0.10835 test_loss: 0.08410 \n",
      "[203/500] train_loss: 0.09107 valid_loss: 0.10868 test_loss: 0.08562 \n",
      "[204/500] train_loss: 0.09036 valid_loss: 0.10812 test_loss: 0.08264 \n",
      "[205/500] train_loss: 0.09328 valid_loss: 0.10760 test_loss: 0.08735 \n",
      "[206/500] train_loss: 0.09109 valid_loss: 0.10668 test_loss: 0.08269 \n",
      "Validation loss decreased (0.107503 --> 0.106675).  Saving model ...\n",
      "[207/500] train_loss: 0.09319 valid_loss: 0.10685 test_loss: 0.08220 \n",
      "[208/500] train_loss: 0.09102 valid_loss: 0.10864 test_loss: 0.08308 \n",
      "[209/500] train_loss: 0.09111 valid_loss: 0.10859 test_loss: 0.08446 \n",
      "[210/500] train_loss: 0.09313 valid_loss: 0.10847 test_loss: 0.08372 \n",
      "[211/500] train_loss: 0.09017 valid_loss: 0.10804 test_loss: 0.08257 \n",
      "[212/500] train_loss: 0.09086 valid_loss: 0.10797 test_loss: 0.08337 \n",
      "[213/500] train_loss: 0.09494 valid_loss: 0.10694 test_loss: 0.08288 \n",
      "[214/500] train_loss: 0.09043 valid_loss: 0.10763 test_loss: 0.08328 \n",
      "[215/500] train_loss: 0.09304 valid_loss: 0.10661 test_loss: 0.08296 \n",
      "Validation loss decreased (0.106675 --> 0.106614).  Saving model ...\n",
      "[216/500] train_loss: 0.08955 valid_loss: 0.10805 test_loss: 0.08376 \n",
      "[217/500] train_loss: 0.09002 valid_loss: 0.10711 test_loss: 0.08251 \n",
      "[218/500] train_loss: 0.09120 valid_loss: 0.10755 test_loss: 0.08217 \n",
      "[219/500] train_loss: 0.08936 valid_loss: 0.10792 test_loss: 0.08398 \n",
      "[220/500] train_loss: 0.08679 valid_loss: 0.10747 test_loss: 0.08307 \n",
      "[221/500] train_loss: 0.09135 valid_loss: 0.10719 test_loss: 0.08393 \n",
      "[222/500] train_loss: 0.09246 valid_loss: 0.10664 test_loss: 0.08190 \n",
      "[223/500] train_loss: 0.08926 valid_loss: 0.10648 test_loss: 0.08336 \n",
      "Validation loss decreased (0.106614 --> 0.106485).  Saving model ...\n",
      "[224/500] train_loss: 0.09126 valid_loss: 0.10644 test_loss: 0.08412 \n",
      "Validation loss decreased (0.106485 --> 0.106439).  Saving model ...\n",
      "[225/500] train_loss: 0.09203 valid_loss: 0.10662 test_loss: 0.08373 \n",
      "[226/500] train_loss: 0.08991 valid_loss: 0.10655 test_loss: 0.08072 \n",
      "[227/500] train_loss: 0.09085 valid_loss: 0.10763 test_loss: 0.08432 \n",
      "[228/500] train_loss: 0.08812 valid_loss: 0.10547 test_loss: 0.08355 \n",
      "Validation loss decreased (0.106439 --> 0.105469).  Saving model ...\n",
      "[229/500] train_loss: 0.08806 valid_loss: 0.10531 test_loss: 0.08110 \n",
      "Validation loss decreased (0.105469 --> 0.105309).  Saving model ...\n",
      "[230/500] train_loss: 0.08967 valid_loss: 0.10520 test_loss: 0.08773 \n",
      "Validation loss decreased (0.105309 --> 0.105197).  Saving model ...\n",
      "[231/500] train_loss: 0.08819 valid_loss: 0.10563 test_loss: 0.08257 \n",
      "[232/500] train_loss: 0.08891 valid_loss: 0.10522 test_loss: 0.08281 \n",
      "[233/500] train_loss: 0.08959 valid_loss: 0.10685 test_loss: 0.08270 \n",
      "[234/500] train_loss: 0.08693 valid_loss: 0.10615 test_loss: 0.08329 \n",
      "[235/500] train_loss: 0.08792 valid_loss: 0.10653 test_loss: 0.08278 \n",
      "[236/500] train_loss: 0.08904 valid_loss: 0.10752 test_loss: 0.08323 \n",
      "[237/500] train_loss: 0.09066 valid_loss: 0.10519 test_loss: 0.08186 \n",
      "Validation loss decreased (0.105197 --> 0.105194).  Saving model ...\n",
      "[238/500] train_loss: 0.08847 valid_loss: 0.10505 test_loss: 0.08737 \n",
      "Validation loss decreased (0.105194 --> 0.105051).  Saving model ...\n",
      "[239/500] train_loss: 0.09040 valid_loss: 0.10620 test_loss: 0.08479 \n",
      "[240/500] train_loss: 0.09027 valid_loss: 0.10594 test_loss: 0.08510 \n",
      "[241/500] train_loss: 0.08782 valid_loss: 0.10488 test_loss: 0.08285 \n",
      "Validation loss decreased (0.105051 --> 0.104883).  Saving model ...\n",
      "[242/500] train_loss: 0.08960 valid_loss: 0.10480 test_loss: 0.08206 \n",
      "Validation loss decreased (0.104883 --> 0.104801).  Saving model ...\n",
      "[243/500] train_loss: 0.09106 valid_loss: 0.10516 test_loss: 0.08440 \n",
      "[244/500] train_loss: 0.08809 valid_loss: 0.10683 test_loss: 0.08649 \n",
      "[245/500] train_loss: 0.08601 valid_loss: 0.10498 test_loss: 0.08101 \n",
      "[246/500] train_loss: 0.09194 valid_loss: 0.10436 test_loss: 0.09149 \n",
      "Validation loss decreased (0.104801 --> 0.104365).  Saving model ...\n",
      "[247/500] train_loss: 0.08674 valid_loss: 0.10590 test_loss: 0.08210 \n",
      "[248/500] train_loss: 0.08935 valid_loss: 0.10449 test_loss: 0.08077 \n",
      "[249/500] train_loss: 0.08987 valid_loss: 0.10491 test_loss: 0.08239 \n",
      "[250/500] train_loss: 0.08910 valid_loss: 0.10463 test_loss: 0.08168 \n",
      "[251/500] train_loss: 0.08920 valid_loss: 0.10357 test_loss: 0.08010 \n",
      "Validation loss decreased (0.104365 --> 0.103567).  Saving model ...\n",
      "[252/500] train_loss: 0.08705 valid_loss: 0.10431 test_loss: 0.08290 \n",
      "[253/500] train_loss: 0.08680 valid_loss: 0.10326 test_loss: 0.08435 \n",
      "Validation loss decreased (0.103567 --> 0.103258).  Saving model ...\n",
      "[254/500] train_loss: 0.08506 valid_loss: 0.10378 test_loss: 0.08001 \n",
      "[255/500] train_loss: 0.08950 valid_loss: 0.10380 test_loss: 0.08188 \n",
      "[256/500] train_loss: 0.08835 valid_loss: 0.10264 test_loss: 0.08430 \n",
      "Validation loss decreased (0.103258 --> 0.102641).  Saving model ...\n",
      "[257/500] train_loss: 0.08457 valid_loss: 0.10306 test_loss: 0.08076 \n",
      "[258/500] train_loss: 0.08847 valid_loss: 0.10352 test_loss: 0.08284 \n",
      "[259/500] train_loss: 0.08558 valid_loss: 0.10342 test_loss: 0.08121 \n",
      "[260/500] train_loss: 0.08655 valid_loss: 0.10354 test_loss: 0.08113 \n",
      "[261/500] train_loss: 0.08508 valid_loss: 0.10411 test_loss: 0.08376 \n",
      "[262/500] train_loss: 0.08477 valid_loss: 0.10212 test_loss: 0.08211 \n",
      "Validation loss decreased (0.102641 --> 0.102115).  Saving model ...\n",
      "[263/500] train_loss: 0.08488 valid_loss: 0.10229 test_loss: 0.08340 \n",
      "[264/500] train_loss: 0.08777 valid_loss: 0.10253 test_loss: 0.08371 \n",
      "[265/500] train_loss: 0.08480 valid_loss: 0.10412 test_loss: 0.08101 \n",
      "[266/500] train_loss: 0.08653 valid_loss: 0.10360 test_loss: 0.08291 \n",
      "[267/500] train_loss: 0.08611 valid_loss: 0.10305 test_loss: 0.08630 \n",
      "[268/500] train_loss: 0.09027 valid_loss: 0.10227 test_loss: 0.07992 \n",
      "[269/500] train_loss: 0.08674 valid_loss: 0.10376 test_loss: 0.08195 \n",
      "[270/500] train_loss: 0.08646 valid_loss: 0.10283 test_loss: 0.08450 \n",
      "[271/500] train_loss: 0.08459 valid_loss: 0.10366 test_loss: 0.08472 \n",
      "[272/500] train_loss: 0.08517 valid_loss: 0.10145 test_loss: 0.08229 \n",
      "Validation loss decreased (0.102115 --> 0.101445).  Saving model ...\n",
      "[273/500] train_loss: 0.08583 valid_loss: 0.10378 test_loss: 0.08242 \n",
      "[274/500] train_loss: 0.08532 valid_loss: 0.10198 test_loss: 0.08082 \n",
      "[275/500] train_loss: 0.08443 valid_loss: 0.10192 test_loss: 0.07878 \n",
      "[276/500] train_loss: 0.08771 valid_loss: 0.10208 test_loss: 0.08144 \n",
      "[277/500] train_loss: 0.08497 valid_loss: 0.10266 test_loss: 0.07884 \n",
      "[278/500] train_loss: 0.08595 valid_loss: 0.10199 test_loss: 0.08610 \n",
      "[279/500] train_loss: 0.08453 valid_loss: 0.10139 test_loss: 0.08308 \n",
      "Validation loss decreased (0.101445 --> 0.101385).  Saving model ...\n",
      "[280/500] train_loss: 0.08809 valid_loss: 0.10117 test_loss: 0.08312 \n",
      "Validation loss decreased (0.101385 --> 0.101169).  Saving model ...\n",
      "[281/500] train_loss: 0.08717 valid_loss: 0.10179 test_loss: 0.08031 \n",
      "[282/500] train_loss: 0.08568 valid_loss: 0.10157 test_loss: 0.08007 \n",
      "[283/500] train_loss: 0.08634 valid_loss: 0.10251 test_loss: 0.08261 \n",
      "[284/500] train_loss: 0.08674 valid_loss: 0.10116 test_loss: 0.08189 \n",
      "Validation loss decreased (0.101169 --> 0.101158).  Saving model ...\n",
      "[285/500] train_loss: 0.08425 valid_loss: 0.10218 test_loss: 0.07993 \n",
      "[286/500] train_loss: 0.08572 valid_loss: 0.10163 test_loss: 0.08235 \n",
      "[287/500] train_loss: 0.08562 valid_loss: 0.10203 test_loss: 0.08297 \n",
      "[288/500] train_loss: 0.08474 valid_loss: 0.10241 test_loss: 0.08045 \n",
      "[289/500] train_loss: 0.08642 valid_loss: 0.10144 test_loss: 0.08098 \n",
      "[290/500] train_loss: 0.08251 valid_loss: 0.10175 test_loss: 0.08299 \n",
      "[291/500] train_loss: 0.08600 valid_loss: 0.10070 test_loss: 0.08102 \n",
      "Validation loss decreased (0.101158 --> 0.100704).  Saving model ...\n",
      "[292/500] train_loss: 0.08447 valid_loss: 0.10079 test_loss: 0.07932 \n",
      "[293/500] train_loss: 0.08568 valid_loss: 0.10244 test_loss: 0.07927 \n",
      "[294/500] train_loss: 0.08500 valid_loss: 0.10329 test_loss: 0.08090 \n",
      "[295/500] train_loss: 0.08470 valid_loss: 0.10105 test_loss: 0.08008 \n",
      "[296/500] train_loss: 0.08238 valid_loss: 0.10015 test_loss: 0.07794 \n",
      "Validation loss decreased (0.100704 --> 0.100152).  Saving model ...\n",
      "[297/500] train_loss: 0.08178 valid_loss: 0.10109 test_loss: 0.07936 \n",
      "[298/500] train_loss: 0.08710 valid_loss: 0.10130 test_loss: 0.08185 \n",
      "[299/500] train_loss: 0.08901 valid_loss: 0.10053 test_loss: 0.08176 \n",
      "[300/500] train_loss: 0.08571 valid_loss: 0.10197 test_loss: 0.07988 \n",
      "[301/500] train_loss: 0.08649 valid_loss: 0.10159 test_loss: 0.08139 \n",
      "[302/500] train_loss: 0.08366 valid_loss: 0.10262 test_loss: 0.08301 \n",
      "[303/500] train_loss: 0.08539 valid_loss: 0.10112 test_loss: 0.08479 \n",
      "[304/500] train_loss: 0.08246 valid_loss: 0.10091 test_loss: 0.07793 \n",
      "[305/500] train_loss: 0.08166 valid_loss: 0.10121 test_loss: 0.08149 \n",
      "[306/500] train_loss: 0.08428 valid_loss: 0.10097 test_loss: 0.08231 \n",
      "[307/500] train_loss: 0.08322 valid_loss: 0.10256 test_loss: 0.08039 \n",
      "[308/500] train_loss: 0.08494 valid_loss: 0.10209 test_loss: 0.07953 \n",
      "[309/500] train_loss: 0.08156 valid_loss: 0.10076 test_loss: 0.07931 \n",
      "[310/500] train_loss: 0.08463 valid_loss: 0.10118 test_loss: 0.07899 \n",
      "[311/500] train_loss: 0.08619 valid_loss: 0.10132 test_loss: 0.08157 \n",
      "[312/500] train_loss: 0.08426 valid_loss: 0.10146 test_loss: 0.08097 \n",
      "[313/500] train_loss: 0.08360 valid_loss: 0.10040 test_loss: 0.07902 \n",
      "[314/500] train_loss: 0.08426 valid_loss: 0.10081 test_loss: 0.09520 \n",
      "[315/500] train_loss: 0.08306 valid_loss: 0.10150 test_loss: 0.08007 \n",
      "[316/500] train_loss: 0.08163 valid_loss: 0.10019 test_loss: 0.09087 \n",
      "[317/500] train_loss: 0.08312 valid_loss: 0.10084 test_loss: 0.09218 \n",
      "[318/500] train_loss: 0.08353 valid_loss: 0.10025 test_loss: 0.08387 \n",
      "[319/500] train_loss: 0.08365 valid_loss: 0.10034 test_loss: 0.08077 \n",
      "[320/500] train_loss: 0.08189 valid_loss: 0.10028 test_loss: 0.08158 \n",
      "[321/500] train_loss: 0.08123 valid_loss: 0.10177 test_loss: 0.08016 \n",
      "[322/500] train_loss: 0.08217 valid_loss: 0.10061 test_loss: 0.07924 \n",
      "[323/500] train_loss: 0.08368 valid_loss: 0.10052 test_loss: 0.07879 \n",
      "[324/500] train_loss: 0.08278 valid_loss: 0.09993 test_loss: 0.08772 \n",
      "Validation loss decreased (0.100152 --> 0.099935).  Saving model ...\n",
      "[325/500] train_loss: 0.08201 valid_loss: 0.10072 test_loss: 0.08317 \n",
      "[326/500] train_loss: 0.08473 valid_loss: 0.09901 test_loss: 0.08414 \n",
      "Validation loss decreased (0.099935 --> 0.099013).  Saving model ...\n",
      "[327/500] train_loss: 0.08058 valid_loss: 0.09867 test_loss: 0.08207 \n",
      "Validation loss decreased (0.099013 --> 0.098675).  Saving model ...\n",
      "[328/500] train_loss: 0.08299 valid_loss: 0.10017 test_loss: 0.07934 \n",
      "[329/500] train_loss: 0.08119 valid_loss: 0.10079 test_loss: 0.08174 \n",
      "[330/500] train_loss: 0.08302 valid_loss: 0.09869 test_loss: 0.08871 \n",
      "[331/500] train_loss: 0.08099 valid_loss: 0.09939 test_loss: 0.08226 \n",
      "[332/500] train_loss: 0.08492 valid_loss: 0.10028 test_loss: 0.08172 \n",
      "[333/500] train_loss: 0.08300 valid_loss: 0.10015 test_loss: 0.07987 \n",
      "[334/500] train_loss: 0.08224 valid_loss: 0.09974 test_loss: 0.07760 \n",
      "[335/500] train_loss: 0.08317 valid_loss: 0.09923 test_loss: 0.08367 \n",
      "[336/500] train_loss: 0.08269 valid_loss: 0.09847 test_loss: 0.07753 \n",
      "Validation loss decreased (0.098675 --> 0.098466).  Saving model ...\n",
      "[337/500] train_loss: 0.08062 valid_loss: 0.09832 test_loss: 0.08460 \n",
      "Validation loss decreased (0.098466 --> 0.098321).  Saving model ...\n",
      "[338/500] train_loss: 0.07946 valid_loss: 0.09833 test_loss: 0.07739 \n",
      "[339/500] train_loss: 0.08355 valid_loss: 0.09931 test_loss: 0.08066 \n",
      "[340/500] train_loss: 0.08190 valid_loss: 0.09907 test_loss: 0.08857 \n",
      "[341/500] train_loss: 0.08189 valid_loss: 0.09900 test_loss: 0.08030 \n",
      "[342/500] train_loss: 0.08281 valid_loss: 0.09932 test_loss: 0.09114 \n",
      "[343/500] train_loss: 0.08177 valid_loss: 0.09805 test_loss: 0.10384 \n",
      "Validation loss decreased (0.098321 --> 0.098053).  Saving model ...\n",
      "[344/500] train_loss: 0.08032 valid_loss: 0.09879 test_loss: 0.07751 \n",
      "[345/500] train_loss: 0.07969 valid_loss: 0.10031 test_loss: 0.08489 \n",
      "[346/500] train_loss: 0.08393 valid_loss: 0.09883 test_loss: 0.08429 \n",
      "[347/500] train_loss: 0.08238 valid_loss: 0.09900 test_loss: 0.08572 \n",
      "[348/500] train_loss: 0.08229 valid_loss: 0.09899 test_loss: 0.08432 \n",
      "[349/500] train_loss: 0.08206 valid_loss: 0.09815 test_loss: 0.07657 \n",
      "[350/500] train_loss: 0.08135 valid_loss: 0.09755 test_loss: 0.07729 \n",
      "Validation loss decreased (0.098053 --> 0.097551).  Saving model ...\n",
      "[351/500] train_loss: 0.07912 valid_loss: 0.09812 test_loss: 0.08293 \n",
      "[352/500] train_loss: 0.08298 valid_loss: 0.09863 test_loss: 0.08293 \n",
      "[353/500] train_loss: 0.08067 valid_loss: 0.09884 test_loss: 0.07723 \n",
      "[354/500] train_loss: 0.07717 valid_loss: 0.09890 test_loss: 0.08604 \n",
      "[355/500] train_loss: 0.08064 valid_loss: 0.09763 test_loss: 0.07507 \n",
      "[356/500] train_loss: 0.08217 valid_loss: 0.09753 test_loss: 0.08056 \n",
      "Validation loss decreased (0.097551 --> 0.097526).  Saving model ...\n",
      "[357/500] train_loss: 0.07939 valid_loss: 0.09840 test_loss: 0.08179 \n",
      "[358/500] train_loss: 0.08143 valid_loss: 0.09838 test_loss: 0.07827 \n",
      "[359/500] train_loss: 0.07925 valid_loss: 0.09714 test_loss: 0.09788 \n",
      "Validation loss decreased (0.097526 --> 0.097143).  Saving model ...\n",
      "[360/500] train_loss: 0.07988 valid_loss: 0.09809 test_loss: 0.08018 \n",
      "[361/500] train_loss: 0.08166 valid_loss: 0.09687 test_loss: 0.08481 \n",
      "Validation loss decreased (0.097143 --> 0.096870).  Saving model ...\n",
      "[362/500] train_loss: 0.07948 valid_loss: 0.09800 test_loss: 0.07734 \n",
      "[363/500] train_loss: 0.08115 valid_loss: 0.09735 test_loss: 0.07651 \n",
      "[364/500] train_loss: 0.07710 valid_loss: 0.09767 test_loss: 0.07715 \n",
      "[365/500] train_loss: 0.08191 valid_loss: 0.09831 test_loss: 0.07672 \n",
      "[366/500] train_loss: 0.08075 valid_loss: 0.09867 test_loss: 0.07842 \n",
      "[367/500] train_loss: 0.08139 valid_loss: 0.09716 test_loss: 0.08253 \n",
      "[368/500] train_loss: 0.08014 valid_loss: 0.09646 test_loss: 0.09543 \n",
      "Validation loss decreased (0.096870 --> 0.096464).  Saving model ...\n",
      "[369/500] train_loss: 0.08050 valid_loss: 0.09762 test_loss: 0.07853 \n",
      "[370/500] train_loss: 0.08013 valid_loss: 0.09805 test_loss: 0.08036 \n",
      "[371/500] train_loss: 0.08107 valid_loss: 0.09617 test_loss: 0.08598 \n",
      "Validation loss decreased (0.096464 --> 0.096175).  Saving model ...\n",
      "[372/500] train_loss: 0.07662 valid_loss: 0.09885 test_loss: 0.08578 \n",
      "[373/500] train_loss: 0.07856 valid_loss: 0.09691 test_loss: 0.09367 \n",
      "[374/500] train_loss: 0.07906 valid_loss: 0.09836 test_loss: 0.07695 \n",
      "[375/500] train_loss: 0.07819 valid_loss: 0.09711 test_loss: 0.07574 \n",
      "[376/500] train_loss: 0.07948 valid_loss: 0.09771 test_loss: 0.08831 \n",
      "[377/500] train_loss: 0.07791 valid_loss: 0.09674 test_loss: 0.07814 \n",
      "[378/500] train_loss: 0.08199 valid_loss: 0.09716 test_loss: 0.08074 \n",
      "[379/500] train_loss: 0.07785 valid_loss: 0.09689 test_loss: 0.08064 \n",
      "[380/500] train_loss: 0.07998 valid_loss: 0.09687 test_loss: 0.07699 \n",
      "[381/500] train_loss: 0.08016 valid_loss: 0.09744 test_loss: 0.08045 \n",
      "[382/500] train_loss: 0.08010 valid_loss: 0.09748 test_loss: 0.08205 \n",
      "[383/500] train_loss: 0.08069 valid_loss: 0.09610 test_loss: 0.09862 \n",
      "Validation loss decreased (0.096175 --> 0.096103).  Saving model ...\n",
      "[384/500] train_loss: 0.07956 valid_loss: 0.09878 test_loss: 0.07737 \n",
      "[385/500] train_loss: 0.07909 valid_loss: 0.09685 test_loss: 0.07705 \n",
      "[386/500] train_loss: 0.07973 valid_loss: 0.09857 test_loss: 0.07705 \n",
      "[387/500] train_loss: 0.08188 valid_loss: 0.09775 test_loss: 0.07642 \n",
      "[388/500] train_loss: 0.07812 valid_loss: 0.09875 test_loss: 0.07766 \n",
      "[389/500] train_loss: 0.07772 valid_loss: 0.09745 test_loss: 0.07474 \n",
      "[390/500] train_loss: 0.07976 valid_loss: 0.09790 test_loss: 0.08078 \n",
      "[391/500] train_loss: 0.07919 valid_loss: 0.09720 test_loss: 0.07639 \n",
      "[392/500] train_loss: 0.08007 valid_loss: 0.09622 test_loss: 0.07596 \n",
      "[393/500] train_loss: 0.07972 valid_loss: 0.09686 test_loss: 0.07631 \n",
      "[394/500] train_loss: 0.07994 valid_loss: 0.09673 test_loss: 0.08627 \n",
      "[395/500] train_loss: 0.07830 valid_loss: 0.09632 test_loss: 0.07715 \n",
      "[396/500] train_loss: 0.08176 valid_loss: 0.09615 test_loss: 0.08098 \n",
      "[397/500] train_loss: 0.07757 valid_loss: 0.09609 test_loss: 0.07520 \n",
      "Validation loss decreased (0.096103 --> 0.096093).  Saving model ...\n",
      "[398/500] train_loss: 0.07866 valid_loss: 0.09675 test_loss: 0.08137 \n",
      "[399/500] train_loss: 0.07914 valid_loss: 0.09674 test_loss: 0.08313 \n",
      "[400/500] train_loss: 0.07692 valid_loss: 0.09609 test_loss: 0.07900 \n",
      "[401/500] train_loss: 0.07973 valid_loss: 0.09604 test_loss: 0.07964 \n",
      "Validation loss decreased (0.096093 --> 0.096043).  Saving model ...\n",
      "[402/500] train_loss: 0.07901 valid_loss: 0.09614 test_loss: 0.09168 \n",
      "[403/500] train_loss: 0.07676 valid_loss: 0.09575 test_loss: 0.10427 \n",
      "Validation loss decreased (0.096043 --> 0.095755).  Saving model ...\n",
      "[404/500] train_loss: 0.07784 valid_loss: 0.09608 test_loss: 0.08252 \n",
      "[405/500] train_loss: 0.07839 valid_loss: 0.09625 test_loss: 0.10291 \n",
      "[406/500] train_loss: 0.07710 valid_loss: 0.09729 test_loss: 0.08255 \n",
      "[407/500] train_loss: 0.07771 valid_loss: 0.09750 test_loss: 0.07809 \n",
      "[408/500] train_loss: 0.07995 valid_loss: 0.09646 test_loss: 0.08071 \n",
      "[409/500] train_loss: 0.07879 valid_loss: 0.09683 test_loss: 0.08227 \n",
      "[410/500] train_loss: 0.07702 valid_loss: 0.09722 test_loss: 0.09343 \n",
      "[411/500] train_loss: 0.07920 valid_loss: 0.09687 test_loss: 0.08353 \n",
      "[412/500] train_loss: 0.07817 valid_loss: 0.09658 test_loss: 0.07672 \n",
      "[413/500] train_loss: 0.07780 valid_loss: 0.09856 test_loss: 0.07721 \n",
      "[414/500] train_loss: 0.08040 valid_loss: 0.09587 test_loss: 0.07658 \n",
      "[415/500] train_loss: 0.07956 valid_loss: 0.09696 test_loss: 0.07730 \n",
      "[416/500] train_loss: 0.07605 valid_loss: 0.09614 test_loss: 0.09256 \n",
      "[417/500] train_loss: 0.07610 valid_loss: 0.09731 test_loss: 0.08557 \n",
      "[418/500] train_loss: 0.07858 valid_loss: 0.09571 test_loss: 0.08370 \n",
      "Validation loss decreased (0.095755 --> 0.095707).  Saving model ...\n",
      "[419/500] train_loss: 0.07795 valid_loss: 0.09522 test_loss: 0.18291 \n",
      "Validation loss decreased (0.095707 --> 0.095222).  Saving model ...\n",
      "[420/500] train_loss: 0.07883 valid_loss: 0.09546 test_loss: 0.15568 \n",
      "[421/500] train_loss: 0.07828 valid_loss: 0.09529 test_loss: 0.09892 \n",
      "[422/500] train_loss: 0.07761 valid_loss: 0.09507 test_loss: 0.10087 \n",
      "Validation loss decreased (0.095222 --> 0.095075).  Saving model ...\n",
      "[423/500] train_loss: 0.07685 valid_loss: 0.09608 test_loss: 0.09253 \n",
      "[424/500] train_loss: 0.08018 valid_loss: 0.09486 test_loss: 0.08687 \n",
      "Validation loss decreased (0.095075 --> 0.094860).  Saving model ...\n",
      "[425/500] train_loss: 0.07646 valid_loss: 0.09489 test_loss: 0.15104 \n",
      "[426/500] train_loss: 0.07649 valid_loss: 0.09566 test_loss: 0.20684 \n",
      "[427/500] train_loss: 0.07750 valid_loss: 0.09544 test_loss: 0.09727 \n",
      "[428/500] train_loss: 0.07622 valid_loss: 0.09436 test_loss: 0.17538 \n",
      "Validation loss decreased (0.094860 --> 0.094358).  Saving model ...\n",
      "[429/500] train_loss: 0.07621 valid_loss: 0.09709 test_loss: 0.14895 \n",
      "[430/500] train_loss: 0.07893 valid_loss: 0.09415 test_loss: 0.13473 \n",
      "Validation loss decreased (0.094358 --> 0.094150).  Saving model ...\n",
      "[431/500] train_loss: 0.07672 valid_loss: 0.09562 test_loss: 0.11215 \n",
      "[432/500] train_loss: 0.07640 valid_loss: 0.09533 test_loss: 0.16500 \n",
      "[433/500] train_loss: 0.07694 valid_loss: 0.09511 test_loss: 0.11096 \n",
      "[434/500] train_loss: 0.07812 valid_loss: 0.09582 test_loss: 0.07998 \n",
      "[435/500] train_loss: 0.07464 valid_loss: 0.09629 test_loss: 0.07652 \n",
      "[436/500] train_loss: 0.07495 valid_loss: 0.09506 test_loss: 0.10075 \n",
      "[437/500] train_loss: 0.07528 valid_loss: 0.09567 test_loss: 0.08725 \n",
      "[438/500] train_loss: 0.07692 valid_loss: 0.09485 test_loss: 0.08005 \n",
      "[439/500] train_loss: 0.07702 valid_loss: 0.09543 test_loss: 0.07806 \n",
      "[440/500] train_loss: 0.08031 valid_loss: 0.09583 test_loss: 0.08441 \n",
      "[441/500] train_loss: 0.07743 valid_loss: 0.09507 test_loss: 0.21535 \n",
      "[442/500] train_loss: 0.07678 valid_loss: 0.09587 test_loss: 0.19351 \n",
      "[443/500] train_loss: 0.07501 valid_loss: 0.09607 test_loss: 0.10545 \n",
      "[444/500] train_loss: 0.07678 valid_loss: 0.09496 test_loss: 0.09893 \n",
      "[445/500] train_loss: 0.07702 valid_loss: 0.09476 test_loss: 0.10477 \n",
      "[446/500] train_loss: 0.07583 valid_loss: 0.09493 test_loss: 0.11590 \n",
      "[447/500] train_loss: 0.07804 valid_loss: 0.09398 test_loss: 0.09827 \n",
      "Validation loss decreased (0.094150 --> 0.093979).  Saving model ...\n",
      "[448/500] train_loss: 0.07875 valid_loss: 0.09479 test_loss: 0.15812 \n",
      "[449/500] train_loss: 0.07717 valid_loss: 0.09380 test_loss: 0.07655 \n",
      "Validation loss decreased (0.093979 --> 0.093796).  Saving model ...\n",
      "[450/500] train_loss: 0.07380 valid_loss: 0.09449 test_loss: 0.07483 \n",
      "[451/500] train_loss: 0.07724 valid_loss: 0.09456 test_loss: 0.07734 \n",
      "[452/500] train_loss: 0.07821 valid_loss: 0.09404 test_loss: 0.08936 \n",
      "[453/500] train_loss: 0.07671 valid_loss: 0.09438 test_loss: 0.07357 \n",
      "[454/500] train_loss: 0.07748 valid_loss: 0.09523 test_loss: 0.09979 \n",
      "[455/500] train_loss: 0.07684 valid_loss: 0.09625 test_loss: 0.07770 \n",
      "[456/500] train_loss: 0.07617 valid_loss: 0.09688 test_loss: 0.07807 \n",
      "[457/500] train_loss: 0.07314 valid_loss: 0.09658 test_loss: 0.07828 \n",
      "[458/500] train_loss: 0.07554 valid_loss: 0.09440 test_loss: 0.07689 \n",
      "[459/500] train_loss: 0.07476 valid_loss: 0.09525 test_loss: 0.09478 \n",
      "[460/500] train_loss: 0.07580 valid_loss: 0.09643 test_loss: 0.07686 \n",
      "[461/500] train_loss: 0.07432 valid_loss: 0.09596 test_loss: 0.07716 \n",
      "[462/500] train_loss: 0.07378 valid_loss: 0.09569 test_loss: 0.07698 \n",
      "[463/500] train_loss: 0.07670 valid_loss: 0.09450 test_loss: 0.07731 \n",
      "[464/500] train_loss: 0.07559 valid_loss: 0.09400 test_loss: 0.13205 \n",
      "[465/500] train_loss: 0.07505 valid_loss: 0.09526 test_loss: 0.14457 \n",
      "[466/500] train_loss: 0.07532 valid_loss: 0.09603 test_loss: 0.08232 \n",
      "[467/500] train_loss: 0.07623 valid_loss: 0.09556 test_loss: 0.07505 \n",
      "[468/500] train_loss: 0.07454 valid_loss: 0.09545 test_loss: 0.09055 \n",
      "[469/500] train_loss: 0.07457 valid_loss: 0.09319 test_loss: 0.07674 \n",
      "Validation loss decreased (0.093796 --> 0.093194).  Saving model ...\n",
      "[470/500] train_loss: 0.07776 valid_loss: 0.09422 test_loss: 0.08174 \n",
      "[471/500] train_loss: 0.07589 valid_loss: 0.09516 test_loss: 0.09446 \n",
      "[472/500] train_loss: 0.07478 valid_loss: 0.09389 test_loss: 0.08867 \n",
      "[473/500] train_loss: 0.07489 valid_loss: 0.09507 test_loss: 0.10348 \n",
      "[474/500] train_loss: 0.07260 valid_loss: 0.09530 test_loss: 0.10963 \n",
      "[475/500] train_loss: 0.07513 valid_loss: 0.09391 test_loss: 0.11778 \n",
      "[476/500] train_loss: 0.07450 valid_loss: 0.09413 test_loss: 0.08488 \n",
      "[477/500] train_loss: 0.07468 valid_loss: 0.09479 test_loss: 0.11154 \n",
      "[478/500] train_loss: 0.07286 valid_loss: 0.09442 test_loss: 0.09879 \n",
      "[479/500] train_loss: 0.07442 valid_loss: 0.09399 test_loss: 0.10886 \n",
      "[480/500] train_loss: 0.07264 valid_loss: 0.09439 test_loss: 0.10724 \n",
      "[481/500] train_loss: 0.07579 valid_loss: 0.09429 test_loss: 0.07723 \n",
      "[482/500] train_loss: 0.07435 valid_loss: 0.09394 test_loss: 0.07865 \n",
      "[483/500] train_loss: 0.07540 valid_loss: 0.09513 test_loss: 0.08036 \n",
      "[484/500] train_loss: 0.07607 valid_loss: 0.09568 test_loss: 0.08545 \n",
      "[485/500] train_loss: 0.07599 valid_loss: 0.09491 test_loss: 0.07797 \n",
      "[486/500] train_loss: 0.07269 valid_loss: 0.09419 test_loss: 0.07763 \n",
      "[487/500] train_loss: 0.07402 valid_loss: 0.09405 test_loss: 0.08410 \n",
      "[488/500] train_loss: 0.07291 valid_loss: 0.09388 test_loss: 0.07748 \n",
      "[489/500] train_loss: 0.07657 valid_loss: 0.09347 test_loss: 0.07830 \n",
      "[490/500] train_loss: 0.07522 valid_loss: 0.09569 test_loss: 0.07517 \n",
      "[491/500] train_loss: 0.07455 valid_loss: 0.09378 test_loss: 0.08051 \n",
      "[492/500] train_loss: 0.07552 valid_loss: 0.09396 test_loss: 0.07630 \n",
      "[493/500] train_loss: 0.07601 valid_loss: 0.09416 test_loss: 0.08039 \n",
      "[494/500] train_loss: 0.07430 valid_loss: 0.09603 test_loss: 0.07749 \n",
      "[495/500] train_loss: 0.07362 valid_loss: 0.09381 test_loss: 0.07528 \n",
      "[496/500] train_loss: 0.07360 valid_loss: 0.09415 test_loss: 0.07465 \n",
      "[497/500] train_loss: 0.07250 valid_loss: 0.09397 test_loss: 0.07454 \n",
      "[498/500] train_loss: 0.07418 valid_loss: 0.09367 test_loss: 0.07717 \n",
      "[499/500] train_loss: 0.07515 valid_loss: 0.09342 test_loss: 0.07547 \n",
      "[500/500] train_loss: 0.07500 valid_loss: 0.09375 test_loss: 0.07673 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 500\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5.E-5)\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "fn='../data/ukdale/network_weights/UKDALE_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cac8f1-b7b1-4e0c-996e-c49a672e5932",
   "metadata": {},
   "source": [
    "# REFIT training network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a85407-aa8c-47d2-9717-7cd596c1579b",
   "metadata": {},
   "source": [
    "## Fridge case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f3726e-9dc1-4e33-9020-3a95333eb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge']\n",
    "THRESHOLD = [20.]\n",
    "MIN_ON = [600. / 60]\n",
    "MIN_OFF = [100. / 60]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b8dee4-cd29-47be-913e-2039a4d93d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses=[2,5,9,12,15]\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/fridge/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1210c891-39d9-4b45-a69e-154fd67c2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i],SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "ds_house_train  = [Power(original_meter[i][:int(0.8*ds_len[i])], \n",
    "                         ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                         ds_status[i][:int(0.8*ds_len[i])],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, True) for i in range(len(houses))]\n",
    "ds_house_valid  = [Power(original_meter[i][int(0.8*ds_len[i]):], \n",
    "                         ds_appliance[i][int(0.8*ds_len[i]):], \n",
    "                         ds_status[i][int(0.8*ds_len[i]):],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_train[i] for i in range(len(houses)-1)])\n",
    "ds_valid      = torch.utils.data.ConcatDataset([ds_house_valid[i] for i in range(len(houses)-1)])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_test = DataLoader(dataset = ds_house_total[-1], batch_size = BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067a4125-0716-48ba-988b-c5a702ff2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/200] train_loss: 0.79067 valid_loss: 0.74895 test_loss: 0.71022 \n",
      "Validation loss decreased (inf --> 0.748947).  Saving model ...\n",
      "[  2/200] train_loss: 0.74336 valid_loss: 0.73062 test_loss: 0.66869 \n",
      "Validation loss decreased (0.748947 --> 0.730620).  Saving model ...\n",
      "[  3/200] train_loss: 0.70634 valid_loss: 0.66657 test_loss: 0.62075 \n",
      "Validation loss decreased (0.730620 --> 0.666571).  Saving model ...\n",
      "[  4/200] train_loss: 0.66884 valid_loss: 0.67297 test_loss: 0.61870 \n",
      "[  5/200] train_loss: 0.64395 valid_loss: 0.64848 test_loss: 0.56347 \n",
      "Validation loss decreased (0.666571 --> 0.648484).  Saving model ...\n",
      "[  6/200] train_loss: 0.63165 valid_loss: 0.69651 test_loss: 0.61628 \n",
      "[  7/200] train_loss: 0.62136 valid_loss: 0.67472 test_loss: 0.59341 \n",
      "[  8/200] train_loss: 0.61342 valid_loss: 0.61578 test_loss: 0.51655 \n",
      "Validation loss decreased (0.648484 --> 0.615778).  Saving model ...\n",
      "[  9/200] train_loss: 0.59939 valid_loss: 0.63544 test_loss: 0.53530 \n",
      "[ 10/200] train_loss: 0.60115 valid_loss: 0.64064 test_loss: 0.53025 \n",
      "[ 11/200] train_loss: 0.59580 valid_loss: 0.61827 test_loss: 0.51119 \n",
      "[ 12/200] train_loss: 0.59427 valid_loss: 0.61368 test_loss: 0.50891 \n",
      "Validation loss decreased (0.615778 --> 0.613684).  Saving model ...\n",
      "[ 13/200] train_loss: 0.58056 valid_loss: 0.57008 test_loss: 0.46643 \n",
      "Validation loss decreased (0.613684 --> 0.570084).  Saving model ...\n",
      "[ 14/200] train_loss: 0.57370 valid_loss: 0.62967 test_loss: 0.49892 \n",
      "[ 15/200] train_loss: 0.57173 valid_loss: 0.61516 test_loss: 0.50288 \n",
      "[ 16/200] train_loss: 0.56486 valid_loss: 0.56940 test_loss: 0.47212 \n",
      "Validation loss decreased (0.570084 --> 0.569402).  Saving model ...\n",
      "[ 17/200] train_loss: 0.55855 valid_loss: 0.54341 test_loss: 0.44539 \n",
      "Validation loss decreased (0.569402 --> 0.543408).  Saving model ...\n",
      "[ 18/200] train_loss: 0.55429 valid_loss: 0.53308 test_loss: 0.44158 \n",
      "Validation loss decreased (0.543408 --> 0.533081).  Saving model ...\n",
      "[ 19/200] train_loss: 0.54461 valid_loss: 0.52329 test_loss: 0.43671 \n",
      "Validation loss decreased (0.533081 --> 0.523287).  Saving model ...\n",
      "[ 20/200] train_loss: 0.55014 valid_loss: 0.52025 test_loss: 0.43694 \n",
      "Validation loss decreased (0.523287 --> 0.520247).  Saving model ...\n",
      "[ 21/200] train_loss: 0.54870 valid_loss: 0.50905 test_loss: 0.44672 \n",
      "Validation loss decreased (0.520247 --> 0.509053).  Saving model ...\n",
      "[ 22/200] train_loss: 0.54502 valid_loss: 0.49882 test_loss: 0.43431 \n",
      "Validation loss decreased (0.509053 --> 0.498820).  Saving model ...\n",
      "[ 23/200] train_loss: 0.53837 valid_loss: 0.49569 test_loss: 0.42654 \n",
      "Validation loss decreased (0.498820 --> 0.495688).  Saving model ...\n",
      "[ 24/200] train_loss: 0.53354 valid_loss: 0.49653 test_loss: 0.42653 \n",
      "[ 25/200] train_loss: 0.53686 valid_loss: 0.49958 test_loss: 0.49075 \n",
      "[ 26/200] train_loss: 0.52734 valid_loss: 0.49577 test_loss: 0.45108 \n",
      "[ 27/200] train_loss: 0.53191 valid_loss: 0.48201 test_loss: 0.44165 \n",
      "Validation loss decreased (0.495688 --> 0.482006).  Saving model ...\n",
      "[ 28/200] train_loss: 0.52857 valid_loss: 0.48226 test_loss: 0.43174 \n",
      "[ 29/200] train_loss: 0.52441 valid_loss: 0.51012 test_loss: 0.43723 \n",
      "[ 30/200] train_loss: 0.51508 valid_loss: 0.48251 test_loss: 0.40347 \n",
      "[ 31/200] train_loss: 0.51153 valid_loss: 0.49073 test_loss: 0.46981 \n",
      "[ 32/200] train_loss: 0.51441 valid_loss: 0.48031 test_loss: 0.43266 \n",
      "Validation loss decreased (0.482006 --> 0.480305).  Saving model ...\n",
      "[ 33/200] train_loss: 0.51585 valid_loss: 0.47997 test_loss: 0.42780 \n",
      "Validation loss decreased (0.480305 --> 0.479972).  Saving model ...\n",
      "[ 34/200] train_loss: 0.52008 valid_loss: 0.49093 test_loss: 0.41040 \n",
      "[ 35/200] train_loss: 0.51305 valid_loss: 0.48691 test_loss: 0.45378 \n",
      "[ 36/200] train_loss: 0.51148 valid_loss: 0.47137 test_loss: 0.43365 \n",
      "Validation loss decreased (0.479972 --> 0.471370).  Saving model ...\n",
      "[ 37/200] train_loss: 0.50825 valid_loss: 0.47810 test_loss: 0.45133 \n",
      "[ 38/200] train_loss: 0.50554 valid_loss: 0.48882 test_loss: 0.47807 \n",
      "[ 39/200] train_loss: 0.50494 valid_loss: 0.46801 test_loss: 0.43886 \n",
      "Validation loss decreased (0.471370 --> 0.468008).  Saving model ...\n",
      "[ 40/200] train_loss: 0.49831 valid_loss: 0.49574 test_loss: 0.41507 \n",
      "[ 41/200] train_loss: 0.50910 valid_loss: 0.46624 test_loss: 0.40246 \n",
      "Validation loss decreased (0.468008 --> 0.466243).  Saving model ...\n",
      "[ 42/200] train_loss: 0.50112 valid_loss: 0.47379 test_loss: 0.40241 \n",
      "[ 43/200] train_loss: 0.49104 valid_loss: 0.46986 test_loss: 0.41937 \n",
      "[ 44/200] train_loss: 0.49170 valid_loss: 0.47072 test_loss: 0.41862 \n",
      "[ 45/200] train_loss: 0.48547 valid_loss: 0.46578 test_loss: 0.38996 \n",
      "Validation loss decreased (0.466243 --> 0.465776).  Saving model ...\n",
      "[ 46/200] train_loss: 0.48748 valid_loss: 0.46159 test_loss: 0.41669 \n",
      "Validation loss decreased (0.465776 --> 0.461592).  Saving model ...\n",
      "[ 47/200] train_loss: 0.48574 valid_loss: 0.46199 test_loss: 0.41565 \n",
      "[ 48/200] train_loss: 0.49580 valid_loss: 0.45629 test_loss: 0.40385 \n",
      "Validation loss decreased (0.461592 --> 0.456292).  Saving model ...\n",
      "[ 49/200] train_loss: 0.48886 valid_loss: 0.46483 test_loss: 0.38885 \n",
      "[ 50/200] train_loss: 0.48476 valid_loss: 0.47616 test_loss: 0.42868 \n",
      "[ 51/200] train_loss: 0.48430 valid_loss: 0.45734 test_loss: 0.39036 \n",
      "[ 52/200] train_loss: 0.47493 valid_loss: 0.46157 test_loss: 0.47287 \n",
      "[ 53/200] train_loss: 0.48467 valid_loss: 0.46410 test_loss: 0.42433 \n",
      "[ 54/200] train_loss: 0.47874 valid_loss: 0.46289 test_loss: 0.44251 \n",
      "[ 55/200] train_loss: 0.46898 valid_loss: 0.46931 test_loss: 0.40606 \n",
      "[ 56/200] train_loss: 0.47404 valid_loss: 0.46281 test_loss: 0.41541 \n",
      "[ 57/200] train_loss: 0.47254 valid_loss: 0.45776 test_loss: 0.39617 \n",
      "[ 58/200] train_loss: 0.47185 valid_loss: 0.46497 test_loss: 0.40746 \n",
      "[ 59/200] train_loss: 0.47059 valid_loss: 0.48811 test_loss: 0.47983 \n",
      "[ 60/200] train_loss: 0.46968 valid_loss: 0.45754 test_loss: 0.37765 \n",
      "[ 61/200] train_loss: 0.46792 valid_loss: 0.46532 test_loss: 0.44595 \n",
      "[ 62/200] train_loss: 0.46703 valid_loss: 0.45382 test_loss: 0.38523 \n",
      "Validation loss decreased (0.456292 --> 0.453822).  Saving model ...\n",
      "[ 63/200] train_loss: 0.46929 valid_loss: 0.45051 test_loss: 0.37080 \n",
      "Validation loss decreased (0.453822 --> 0.450514).  Saving model ...\n",
      "[ 64/200] train_loss: 0.47788 valid_loss: 0.46308 test_loss: 0.37879 \n",
      "[ 65/200] train_loss: 0.47020 valid_loss: 0.46401 test_loss: 0.40492 \n",
      "[ 66/200] train_loss: 0.46425 valid_loss: 0.45654 test_loss: 0.36558 \n",
      "[ 67/200] train_loss: 0.45889 valid_loss: 0.45757 test_loss: 0.38688 \n",
      "[ 68/200] train_loss: 0.46584 valid_loss: 0.45183 test_loss: 0.39735 \n",
      "[ 69/200] train_loss: 0.46619 valid_loss: 0.43871 test_loss: 0.36394 \n",
      "Validation loss decreased (0.450514 --> 0.438706).  Saving model ...\n",
      "[ 70/200] train_loss: 0.46293 valid_loss: 0.44456 test_loss: 0.35069 \n",
      "[ 71/200] train_loss: 0.46704 valid_loss: 0.46339 test_loss: 0.37382 \n",
      "[ 72/200] train_loss: 0.46237 valid_loss: 0.45616 test_loss: 0.39939 \n",
      "[ 73/200] train_loss: 0.46381 valid_loss: 0.45820 test_loss: 0.36775 \n",
      "[ 74/200] train_loss: 0.47124 valid_loss: 0.45285 test_loss: 0.35046 \n",
      "[ 75/200] train_loss: 0.45682 valid_loss: 0.47612 test_loss: 0.36539 \n",
      "[ 76/200] train_loss: 0.45194 valid_loss: 0.50373 test_loss: 0.37431 \n",
      "[ 77/200] train_loss: 0.45069 valid_loss: 0.44583 test_loss: 0.35241 \n",
      "[ 78/200] train_loss: 0.45742 valid_loss: 0.44366 test_loss: 0.35286 \n",
      "[ 79/200] train_loss: 0.45294 valid_loss: 0.44852 test_loss: 0.35107 \n",
      "[ 80/200] train_loss: 0.44136 valid_loss: 0.44693 test_loss: 0.35595 \n",
      "[ 81/200] train_loss: 0.45272 valid_loss: 0.45157 test_loss: 0.34758 \n",
      "[ 82/200] train_loss: 0.44142 valid_loss: 0.43765 test_loss: 0.35920 \n",
      "Validation loss decreased (0.438706 --> 0.437654).  Saving model ...\n",
      "[ 83/200] train_loss: 0.44986 valid_loss: 0.44561 test_loss: 0.35520 \n",
      "[ 84/200] train_loss: 0.45039 valid_loss: 0.47722 test_loss: 0.37317 \n",
      "[ 85/200] train_loss: 0.45351 valid_loss: 0.44634 test_loss: 0.34995 \n",
      "[ 86/200] train_loss: 0.44460 valid_loss: 0.45019 test_loss: 0.37169 \n",
      "[ 87/200] train_loss: 0.45166 valid_loss: 0.46775 test_loss: 0.34577 \n",
      "[ 88/200] train_loss: 0.44825 valid_loss: 0.45567 test_loss: 0.36046 \n",
      "[ 89/200] train_loss: 0.43824 valid_loss: 0.46974 test_loss: 0.35129 \n",
      "[ 90/200] train_loss: 0.44126 valid_loss: 0.45059 test_loss: 0.34678 \n",
      "[ 91/200] train_loss: 0.43557 valid_loss: 0.47252 test_loss: 0.34384 \n",
      "[ 92/200] train_loss: 0.44760 valid_loss: 0.46622 test_loss: 0.35791 \n",
      "[ 93/200] train_loss: 0.44046 valid_loss: 0.44339 test_loss: 0.36771 \n",
      "[ 94/200] train_loss: 0.43556 valid_loss: 0.48183 test_loss: 0.35949 \n",
      "[ 95/200] train_loss: 0.44818 valid_loss: 0.44247 test_loss: 0.34439 \n",
      "[ 96/200] train_loss: 0.43950 valid_loss: 0.44443 test_loss: 0.33975 \n",
      "[ 97/200] train_loss: 0.44356 valid_loss: 0.46490 test_loss: 0.34582 \n",
      "[ 98/200] train_loss: 0.44001 valid_loss: 0.45708 test_loss: 0.34676 \n",
      "[ 99/200] train_loss: 0.43984 valid_loss: 0.43997 test_loss: 0.33877 \n",
      "[100/200] train_loss: 0.44190 valid_loss: 0.49304 test_loss: 0.36601 \n",
      "[101/200] train_loss: 0.43957 valid_loss: 0.45090 test_loss: 0.34147 \n",
      "[102/200] train_loss: 0.43224 valid_loss: 0.44397 test_loss: 0.34565 \n",
      "[103/200] train_loss: 0.42750 valid_loss: 0.43718 test_loss: 0.35380 \n",
      "Validation loss decreased (0.437654 --> 0.437176).  Saving model ...\n",
      "[104/200] train_loss: 0.43547 valid_loss: 0.44494 test_loss: 0.35538 \n",
      "[105/200] train_loss: 0.43610 valid_loss: 0.44875 test_loss: 0.36403 \n",
      "[106/200] train_loss: 0.43114 valid_loss: 0.44480 test_loss: 0.33794 \n",
      "[107/200] train_loss: 0.43524 valid_loss: 0.44674 test_loss: 0.34024 \n",
      "[108/200] train_loss: 0.42536 valid_loss: 0.44816 test_loss: 0.33364 \n",
      "[109/200] train_loss: 0.42456 valid_loss: 0.44007 test_loss: 0.34484 \n",
      "[110/200] train_loss: 0.43375 valid_loss: 0.43970 test_loss: 0.34205 \n",
      "[111/200] train_loss: 0.42449 valid_loss: 0.43941 test_loss: 0.33873 \n",
      "[112/200] train_loss: 0.42144 valid_loss: 0.44047 test_loss: 0.32498 \n",
      "[113/200] train_loss: 0.42710 valid_loss: 0.48515 test_loss: 0.34950 \n",
      "[114/200] train_loss: 0.42974 valid_loss: 0.44424 test_loss: 0.33504 \n",
      "[115/200] train_loss: 0.42511 valid_loss: 0.45788 test_loss: 0.32996 \n",
      "[116/200] train_loss: 0.42847 valid_loss: 0.43881 test_loss: 0.32736 \n",
      "[117/200] train_loss: 0.43199 valid_loss: 0.43945 test_loss: 0.32693 \n",
      "[118/200] train_loss: 0.43753 valid_loss: 0.45269 test_loss: 0.33549 \n",
      "[119/200] train_loss: 0.42961 valid_loss: 0.46999 test_loss: 0.32945 \n",
      "[120/200] train_loss: 0.42543 valid_loss: 0.45668 test_loss: 0.33309 \n",
      "[121/200] train_loss: 0.42851 valid_loss: 0.46816 test_loss: 0.33005 \n",
      "[122/200] train_loss: 0.42320 valid_loss: 0.47863 test_loss: 0.34201 \n",
      "[123/200] train_loss: 0.41850 valid_loss: 0.45727 test_loss: 0.32794 \n",
      "[124/200] train_loss: 0.42374 valid_loss: 0.44710 test_loss: 0.32335 \n",
      "[125/200] train_loss: 0.42178 valid_loss: 0.45114 test_loss: 0.33748 \n",
      "[126/200] train_loss: 0.42752 valid_loss: 0.44181 test_loss: 0.32288 \n",
      "[127/200] train_loss: 0.42828 valid_loss: 0.42744 test_loss: 0.32071 \n",
      "Validation loss decreased (0.437176 --> 0.427441).  Saving model ...\n",
      "[128/200] train_loss: 0.41939 valid_loss: 0.44092 test_loss: 0.33217 \n",
      "[129/200] train_loss: 0.41971 valid_loss: 0.44968 test_loss: 0.32097 \n",
      "[130/200] train_loss: 0.42626 valid_loss: 0.46919 test_loss: 0.32951 \n",
      "[131/200] train_loss: 0.41815 valid_loss: 0.44823 test_loss: 0.33771 \n",
      "[132/200] train_loss: 0.42509 valid_loss: 0.42599 test_loss: 0.32373 \n",
      "Validation loss decreased (0.427441 --> 0.425988).  Saving model ...\n",
      "[133/200] train_loss: 0.41926 valid_loss: 0.44602 test_loss: 0.33171 \n",
      "[134/200] train_loss: 0.41844 valid_loss: 0.47988 test_loss: 0.32697 \n",
      "[135/200] train_loss: 0.41687 valid_loss: 0.45761 test_loss: 0.32915 \n",
      "[136/200] train_loss: 0.42222 valid_loss: 0.44720 test_loss: 0.33920 \n",
      "[137/200] train_loss: 0.42147 valid_loss: 0.44740 test_loss: 0.32745 \n",
      "[138/200] train_loss: 0.41966 valid_loss: 0.44402 test_loss: 0.32960 \n",
      "[139/200] train_loss: 0.41652 valid_loss: 0.43596 test_loss: 0.32606 \n",
      "[140/200] train_loss: 0.41804 valid_loss: 0.43794 test_loss: 0.33932 \n",
      "[141/200] train_loss: 0.40795 valid_loss: 0.44363 test_loss: 0.31825 \n",
      "[142/200] train_loss: 0.40996 valid_loss: 0.44580 test_loss: 0.32255 \n",
      "[143/200] train_loss: 0.41539 valid_loss: 0.47344 test_loss: 0.33771 \n",
      "[144/200] train_loss: 0.41654 valid_loss: 0.44768 test_loss: 0.31708 \n",
      "[145/200] train_loss: 0.41121 valid_loss: 0.52761 test_loss: 0.35151 \n",
      "[146/200] train_loss: 0.40769 valid_loss: 0.50031 test_loss: 0.34445 \n",
      "[147/200] train_loss: 0.41856 valid_loss: 0.47518 test_loss: 0.33687 \n",
      "[148/200] train_loss: 0.41064 valid_loss: 0.54271 test_loss: 0.36119 \n",
      "[149/200] train_loss: 0.41783 valid_loss: 0.45438 test_loss: 0.32300 \n",
      "[150/200] train_loss: 0.40607 valid_loss: 0.47488 test_loss: 0.32979 \n",
      "[151/200] train_loss: 0.41449 valid_loss: 0.44548 test_loss: 0.31330 \n",
      "[152/200] train_loss: 0.40617 valid_loss: 0.49152 test_loss: 0.33196 \n",
      "[153/200] train_loss: 0.40814 valid_loss: 0.47987 test_loss: 0.32960 \n",
      "[154/200] train_loss: 0.41484 valid_loss: 0.44380 test_loss: 0.32054 \n",
      "[155/200] train_loss: 0.41574 valid_loss: 0.47358 test_loss: 0.32944 \n",
      "[156/200] train_loss: 0.40819 valid_loss: 0.43285 test_loss: 0.31429 \n",
      "[157/200] train_loss: 0.40267 valid_loss: 0.46934 test_loss: 0.31939 \n",
      "[158/200] train_loss: 0.40873 valid_loss: 0.44748 test_loss: 0.31609 \n",
      "[159/200] train_loss: 0.40333 valid_loss: 0.50405 test_loss: 0.35000 \n",
      "[160/200] train_loss: 0.40789 valid_loss: 0.44290 test_loss: 0.31680 \n",
      "[161/200] train_loss: 0.40527 valid_loss: 0.51505 test_loss: 0.35058 \n",
      "[162/200] train_loss: 0.40933 valid_loss: 0.49781 test_loss: 0.34064 \n",
      "[163/200] train_loss: 0.40512 valid_loss: 0.46663 test_loss: 0.32008 \n",
      "[164/200] train_loss: 0.40709 valid_loss: 0.45426 test_loss: 0.32421 \n",
      "[165/200] train_loss: 0.39937 valid_loss: 0.45606 test_loss: 0.31907 \n",
      "[166/200] train_loss: 0.40098 valid_loss: 0.43253 test_loss: 0.31081 \n",
      "[167/200] train_loss: 0.40606 valid_loss: 0.46706 test_loss: 0.31567 \n",
      "[168/200] train_loss: 0.41004 valid_loss: 0.45499 test_loss: 0.32757 \n",
      "[169/200] train_loss: 0.40183 valid_loss: 0.43937 test_loss: 0.30806 \n",
      "[170/200] train_loss: 0.40150 valid_loss: 0.48779 test_loss: 0.34284 \n",
      "[171/200] train_loss: 0.40713 valid_loss: 0.44819 test_loss: 0.31417 \n",
      "[172/200] train_loss: 0.40261 valid_loss: 0.50956 test_loss: 0.34738 \n",
      "[173/200] train_loss: 0.40531 valid_loss: 0.44502 test_loss: 0.31633 \n",
      "[174/200] train_loss: 0.40506 valid_loss: 0.47644 test_loss: 0.33498 \n",
      "[175/200] train_loss: 0.40229 valid_loss: 0.57665 test_loss: 0.37468 \n",
      "[176/200] train_loss: 0.40804 valid_loss: 0.48137 test_loss: 0.33143 \n",
      "[177/200] train_loss: 0.40796 valid_loss: 0.51010 test_loss: 0.34213 \n",
      "[178/200] train_loss: 0.40027 valid_loss: 0.67162 test_loss: 0.42569 \n",
      "[179/200] train_loss: 0.40306 valid_loss: 0.49019 test_loss: 0.33061 \n",
      "[180/200] train_loss: 0.40056 valid_loss: 0.51378 test_loss: 0.34526 \n",
      "[181/200] train_loss: 0.40453 valid_loss: 0.46229 test_loss: 0.31972 \n",
      "[182/200] train_loss: 0.39381 valid_loss: 0.55063 test_loss: 0.36097 \n",
      "[183/200] train_loss: 0.38982 valid_loss: 0.44479 test_loss: 0.34773 \n",
      "[184/200] train_loss: 0.40148 valid_loss: 0.47853 test_loss: 0.33318 \n",
      "[185/200] train_loss: 0.38903 valid_loss: 0.44533 test_loss: 0.31098 \n",
      "[186/200] train_loss: 0.39644 valid_loss: 0.47930 test_loss: 0.32511 \n",
      "[187/200] train_loss: 0.39902 valid_loss: 0.52535 test_loss: 0.35380 \n",
      "[188/200] train_loss: 0.40944 valid_loss: 0.45798 test_loss: 0.31668 \n",
      "[189/200] train_loss: 0.39968 valid_loss: 0.49667 test_loss: 0.33365 \n",
      "[190/200] train_loss: 0.39146 valid_loss: 0.45081 test_loss: 0.31328 \n",
      "[191/200] train_loss: 0.39611 valid_loss: 0.51573 test_loss: 0.34234 \n",
      "[192/200] train_loss: 0.39542 valid_loss: 0.48091 test_loss: 0.31875 \n",
      "[193/200] train_loss: 0.40031 valid_loss: 0.49230 test_loss: 0.33753 \n",
      "[194/200] train_loss: 0.40332 valid_loss: 0.48645 test_loss: 0.32487 \n",
      "[195/200] train_loss: 0.39732 valid_loss: 0.53875 test_loss: 0.35954 \n",
      "[196/200] train_loss: 0.39927 valid_loss: 0.46530 test_loss: 0.31775 \n",
      "[197/200] train_loss: 0.39613 valid_loss: 0.45648 test_loss: 0.31711 \n",
      "[198/200] train_loss: 0.39554 valid_loss: 0.45444 test_loss: 0.32124 \n",
      "[199/200] train_loss: 0.39346 valid_loss: 0.46881 test_loss: 0.31624 \n",
      "[200/200] train_loss: 0.40005 valid_loss: 0.49063 test_loss: 0.31660 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 200\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5.E-5)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([1.5])).cuda()\n",
    "fn = '../data/refit/network_weights/REFIT_fridge_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbe634-fca1-47aa-8ca3-46a220582dae",
   "metadata": {},
   "source": [
    "## Dishwasher case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcb0c38-d8ff-4c3f-af11-00957bb64835",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['dishwasher']\n",
    "THRESHOLD = [30.]\n",
    "MIN_ON = [1400. / 60]\n",
    "MIN_OFF = [1600. / 60]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49035a44-156d-49fe-a18c-57f909dde269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses=[1,3,5,6,7,9,10,11,13,15,16,18,20,2]\n",
    "\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/dishwasher/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5142a81-8980-4e1b-a83b-a900467b94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i],SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "ds_house_train  = [Power(original_meter[i][:int(0.8*ds_len[i])], \n",
    "                         ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                         ds_status[i][:int(0.8*ds_len[i])],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, True) for i in range(len(houses))]\n",
    "ds_house_valid  = [Power(original_meter[i][int(0.8*ds_len[i]):], \n",
    "                         ds_appliance[i][int(0.8*ds_len[i]):], \n",
    "                         ds_status[i][int(0.8*ds_len[i]):],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_train[i] for i in range(len(houses)-1)])\n",
    "ds_valid      = torch.utils.data.ConcatDataset([ds_house_valid[i] for i in range(len(houses)-1)])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_test = DataLoader(dataset = ds_house_total[-1], batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c95155d-2d79-45f1-8429-f29bea064164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.28475 valid_loss: 0.11968 test_loss: 0.20956 \n",
      "Validation loss decreased (inf --> 0.119684).  Saving model ...\n",
      "[  2/100] train_loss: 0.14600 valid_loss: 0.09399 test_loss: 0.17118 \n",
      "Validation loss decreased (0.119684 --> 0.093994).  Saving model ...\n",
      "[  3/100] train_loss: 0.13236 valid_loss: 0.08466 test_loss: 0.14345 \n",
      "Validation loss decreased (0.093994 --> 0.084663).  Saving model ...\n",
      "[  4/100] train_loss: 0.12355 valid_loss: 0.08342 test_loss: 0.13584 \n",
      "Validation loss decreased (0.084663 --> 0.083422).  Saving model ...\n",
      "[  5/100] train_loss: 0.12522 valid_loss: 0.08108 test_loss: 0.13313 \n",
      "Validation loss decreased (0.083422 --> 0.081081).  Saving model ...\n",
      "[  6/100] train_loss: 0.12170 valid_loss: 0.08034 test_loss: 0.12393 \n",
      "Validation loss decreased (0.081081 --> 0.080339).  Saving model ...\n",
      "[  7/100] train_loss: 0.11863 valid_loss: 0.07584 test_loss: 0.11985 \n",
      "Validation loss decreased (0.080339 --> 0.075841).  Saving model ...\n",
      "[  8/100] train_loss: 0.11050 valid_loss: 0.07297 test_loss: 0.11693 \n",
      "Validation loss decreased (0.075841 --> 0.072968).  Saving model ...\n",
      "[  9/100] train_loss: 0.11038 valid_loss: 0.07386 test_loss: 0.11232 \n",
      "[ 10/100] train_loss: 0.11170 valid_loss: 0.07343 test_loss: 0.11085 \n",
      "[ 11/100] train_loss: 0.11262 valid_loss: 0.07010 test_loss: 0.10737 \n",
      "Validation loss decreased (0.072968 --> 0.070097).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10953 valid_loss: 0.07320 test_loss: 0.10946 \n",
      "[ 13/100] train_loss: 0.10440 valid_loss: 0.07170 test_loss: 0.10383 \n",
      "[ 14/100] train_loss: 0.10946 valid_loss: 0.07304 test_loss: 0.10574 \n",
      "[ 15/100] train_loss: 0.10998 valid_loss: 0.06702 test_loss: 0.10137 \n",
      "Validation loss decreased (0.070097 --> 0.067015).  Saving model ...\n",
      "[ 16/100] train_loss: 0.10859 valid_loss: 0.06943 test_loss: 0.10146 \n",
      "[ 17/100] train_loss: 0.10956 valid_loss: 0.06753 test_loss: 0.09821 \n",
      "[ 18/100] train_loss: 0.10614 valid_loss: 0.07066 test_loss: 0.09888 \n",
      "[ 19/100] train_loss: 0.10823 valid_loss: 0.06794 test_loss: 0.10078 \n",
      "[ 20/100] train_loss: 0.10948 valid_loss: 0.06979 test_loss: 0.09966 \n",
      "[ 21/100] train_loss: 0.10652 valid_loss: 0.06618 test_loss: 0.09652 \n",
      "Validation loss decreased (0.067015 --> 0.066179).  Saving model ...\n",
      "[ 22/100] train_loss: 0.10269 valid_loss: 0.07180 test_loss: 0.10172 \n",
      "[ 23/100] train_loss: 0.10010 valid_loss: 0.06553 test_loss: 0.09526 \n",
      "Validation loss decreased (0.066179 --> 0.065530).  Saving model ...\n",
      "[ 24/100] train_loss: 0.10053 valid_loss: 0.06955 test_loss: 0.09699 \n",
      "[ 25/100] train_loss: 0.10442 valid_loss: 0.06795 test_loss: 0.09602 \n",
      "[ 26/100] train_loss: 0.09444 valid_loss: 0.06743 test_loss: 0.09471 \n",
      "[ 27/100] train_loss: 0.09755 valid_loss: 0.06186 test_loss: 0.09042 \n",
      "Validation loss decreased (0.065530 --> 0.061857).  Saving model ...\n",
      "[ 28/100] train_loss: 0.09517 valid_loss: 0.06454 test_loss: 0.09156 \n",
      "[ 29/100] train_loss: 0.09719 valid_loss: 0.06034 test_loss: 0.08975 \n",
      "Validation loss decreased (0.061857 --> 0.060345).  Saving model ...\n",
      "[ 30/100] train_loss: 0.09473 valid_loss: 0.06740 test_loss: 0.09377 \n",
      "[ 31/100] train_loss: 0.09855 valid_loss: 0.06387 test_loss: 0.09459 \n",
      "[ 32/100] train_loss: 0.09027 valid_loss: 0.06405 test_loss: 0.08974 \n",
      "[ 33/100] train_loss: 0.08736 valid_loss: 0.06341 test_loss: 0.09250 \n",
      "[ 34/100] train_loss: 0.09258 valid_loss: 0.06547 test_loss: 0.09196 \n",
      "[ 35/100] train_loss: 0.09021 valid_loss: 0.06351 test_loss: 0.09007 \n",
      "[ 36/100] train_loss: 0.09517 valid_loss: 0.06519 test_loss: 0.09319 \n",
      "[ 37/100] train_loss: 0.09231 valid_loss: 0.06516 test_loss: 0.09086 \n",
      "[ 38/100] train_loss: 0.08505 valid_loss: 0.06101 test_loss: 0.08823 \n",
      "[ 39/100] train_loss: 0.09141 valid_loss: 0.06003 test_loss: 0.08907 \n",
      "Validation loss decreased (0.060345 --> 0.060033).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08885 valid_loss: 0.06448 test_loss: 0.09211 \n",
      "[ 41/100] train_loss: 0.08964 valid_loss: 0.06159 test_loss: 0.09310 \n",
      "[ 42/100] train_loss: 0.08714 valid_loss: 0.06007 test_loss: 0.08780 \n",
      "[ 43/100] train_loss: 0.08488 valid_loss: 0.06400 test_loss: 0.09246 \n",
      "[ 44/100] train_loss: 0.08469 valid_loss: 0.05998 test_loss: 0.08674 \n",
      "Validation loss decreased (0.060033 --> 0.059982).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08001 valid_loss: 0.06206 test_loss: 0.08820 \n",
      "[ 46/100] train_loss: 0.08329 valid_loss: 0.06107 test_loss: 0.08917 \n",
      "[ 47/100] train_loss: 0.08753 valid_loss: 0.06444 test_loss: 0.08941 \n",
      "[ 48/100] train_loss: 0.08659 valid_loss: 0.06082 test_loss: 0.08554 \n",
      "[ 49/100] train_loss: 0.08515 valid_loss: 0.06754 test_loss: 0.09438 \n",
      "[ 50/100] train_loss: 0.08423 valid_loss: 0.06194 test_loss: 0.09082 \n",
      "[ 51/100] train_loss: 0.07871 valid_loss: 0.06004 test_loss: 0.08601 \n",
      "[ 52/100] train_loss: 0.07915 valid_loss: 0.05803 test_loss: 0.08941 \n",
      "Validation loss decreased (0.059982 --> 0.058034).  Saving model ...\n",
      "[ 53/100] train_loss: 0.07975 valid_loss: 0.05871 test_loss: 0.08711 \n",
      "[ 54/100] train_loss: 0.08178 valid_loss: 0.06013 test_loss: 0.09638 \n",
      "[ 55/100] train_loss: 0.07508 valid_loss: 0.06506 test_loss: 0.09368 \n",
      "[ 56/100] train_loss: 0.08216 valid_loss: 0.06040 test_loss: 0.09248 \n",
      "[ 57/100] train_loss: 0.07632 valid_loss: 0.06066 test_loss: 0.08880 \n",
      "[ 58/100] train_loss: 0.07707 valid_loss: 0.05785 test_loss: 0.08890 \n",
      "Validation loss decreased (0.058034 --> 0.057854).  Saving model ...\n",
      "[ 59/100] train_loss: 0.07489 valid_loss: 0.06208 test_loss: 0.09031 \n",
      "[ 60/100] train_loss: 0.07793 valid_loss: 0.06171 test_loss: 0.09071 \n",
      "[ 61/100] train_loss: 0.07544 valid_loss: 0.06087 test_loss: 0.08845 \n",
      "[ 62/100] train_loss: 0.07374 valid_loss: 0.06107 test_loss: 0.09110 \n",
      "[ 63/100] train_loss: 0.08019 valid_loss: 0.06525 test_loss: 0.08780 \n",
      "[ 64/100] train_loss: 0.07856 valid_loss: 0.05919 test_loss: 0.08949 \n",
      "[ 65/100] train_loss: 0.07869 valid_loss: 0.06068 test_loss: 0.08935 \n",
      "[ 66/100] train_loss: 0.07840 valid_loss: 0.06098 test_loss: 0.09081 \n",
      "[ 67/100] train_loss: 0.07200 valid_loss: 0.06333 test_loss: 0.09117 \n",
      "[ 68/100] train_loss: 0.07682 valid_loss: 0.06391 test_loss: 0.09408 \n",
      "[ 69/100] train_loss: 0.07184 valid_loss: 0.06170 test_loss: 0.09036 \n",
      "[ 70/100] train_loss: 0.07575 valid_loss: 0.05877 test_loss: 0.08765 \n",
      "[ 71/100] train_loss: 0.07709 valid_loss: 0.06717 test_loss: 0.09607 \n",
      "[ 72/100] train_loss: 0.07507 valid_loss: 0.06489 test_loss: 0.09739 \n",
      "[ 73/100] train_loss: 0.06972 valid_loss: 0.05934 test_loss: 0.08723 \n",
      "[ 74/100] train_loss: 0.07252 valid_loss: 0.06314 test_loss: 0.09182 \n",
      "[ 75/100] train_loss: 0.07399 valid_loss: 0.06146 test_loss: 0.09187 \n",
      "[ 76/100] train_loss: 0.07259 valid_loss: 0.06091 test_loss: 0.08558 \n",
      "[ 77/100] train_loss: 0.07140 valid_loss: 0.06679 test_loss: 0.10063 \n",
      "[ 78/100] train_loss: 0.07344 valid_loss: 0.06261 test_loss: 0.08714 \n",
      "[ 79/100] train_loss: 0.07145 valid_loss: 0.06321 test_loss: 0.09155 \n",
      "[ 80/100] train_loss: 0.06891 valid_loss: 0.06439 test_loss: 0.09632 \n",
      "[ 81/100] train_loss: 0.06571 valid_loss: 0.06131 test_loss: 0.08954 \n",
      "[ 82/100] train_loss: 0.07016 valid_loss: 0.05879 test_loss: 0.08766 \n",
      "[ 83/100] train_loss: 0.07262 valid_loss: 0.07092 test_loss: 0.09557 \n",
      "[ 84/100] train_loss: 0.06768 valid_loss: 0.06170 test_loss: 0.09044 \n",
      "[ 85/100] train_loss: 0.06769 valid_loss: 0.06322 test_loss: 0.09239 \n",
      "[ 86/100] train_loss: 0.06668 valid_loss: 0.06633 test_loss: 0.09474 \n",
      "[ 87/100] train_loss: 0.06693 valid_loss: 0.05971 test_loss: 0.08785 \n",
      "[ 88/100] train_loss: 0.06983 valid_loss: 0.05937 test_loss: 0.09078 \n",
      "[ 89/100] train_loss: 0.06770 valid_loss: 0.06396 test_loss: 0.09096 \n",
      "[ 90/100] train_loss: 0.07048 valid_loss: 0.06117 test_loss: 0.09402 \n",
      "[ 91/100] train_loss: 0.06518 valid_loss: 0.06258 test_loss: 0.08832 \n",
      "[ 92/100] train_loss: 0.06878 valid_loss: 0.06426 test_loss: 0.09473 \n",
      "[ 93/100] train_loss: 0.07000 valid_loss: 0.06450 test_loss: 0.08824 \n",
      "[ 94/100] train_loss: 0.06371 valid_loss: 0.06697 test_loss: 0.09086 \n",
      "[ 95/100] train_loss: 0.06526 valid_loss: 0.05990 test_loss: 0.08732 \n",
      "[ 96/100] train_loss: 0.06199 valid_loss: 0.06535 test_loss: 0.09702 \n",
      "[ 97/100] train_loss: 0.06390 valid_loss: 0.06720 test_loss: 0.09356 \n",
      "[ 98/100] train_loss: 0.06740 valid_loss: 0.06181 test_loss: 0.09460 \n",
      "[ 99/100] train_loss: 0.06654 valid_loss: 0.06746 test_loss: 0.09120 \n",
      "[100/100] train_loss: 0.06790 valid_loss: 0.07005 test_loss: 0.09833 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5.E-5)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([1.9])).cuda()\n",
    "fn = '../data/refit/network_weights/REFIT_dishwasher_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e040173-5751-4b4b-9566-5f01716adb57",
   "metadata": {},
   "source": [
    "## Washing-machine case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f74515ab-9442-40f4-bb6a-264782bd3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['washing_machine']\n",
    "THRESHOLD = [20.]\n",
    "MIN_ON = [1600. / 60]\n",
    "MIN_OFF = [800. / 60]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cccfbc7-7b3a-4c13-be3f-017d2f4a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_status\n",
    "original_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "houses = [1,2,3,5,7,8,9,10,11,13,15,16,17,18,19,20,21,6]\n",
    "\n",
    "\n",
    "for i in houses:\n",
    "    ds = pd.read_feather('../data/refit/feather_files/washing_machine/REFIT_%d.feather' %i)\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    original_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(original_meter[i]) for i in range(len(houses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3537cf55-30c0-4091-8bda-8bc2e24d0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import Power\n",
    "\n",
    "ds_house_total  = [Power(original_meter[i], ds_appliance[i], ds_status[i],SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "ds_house_train  = [Power(original_meter[i][:int(0.8*ds_len[i])], \n",
    "                         ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                         ds_status[i][:int(0.8*ds_len[i])],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, True) for i in range(len(houses))]\n",
    "ds_house_valid  = [Power(original_meter[i][int(0.8*ds_len[i]):], \n",
    "                         ds_appliance[i][int(0.8*ds_len[i]):], \n",
    "                         ds_status[i][int(0.8*ds_len[i]):],\n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(len(houses))]\n",
    "\n",
    "ds_train      = torch.utils.data.ConcatDataset([ds_house_train[i] for i in range(len(houses)-1)])\n",
    "ds_valid      = torch.utils.data.ConcatDataset([ds_house_valid[i] for i in range(len(houses)-1)])\n",
    "\n",
    "dl_train = DataLoader(dataset = ds_train, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_valid = DataLoader(dataset = ds_valid, batch_size = BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dl_test = DataLoader(dataset = ds_house_total[-1], batch_size = BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e197fdf4-b3bd-4419-81bf-752687dcac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/200] train_loss: 0.31964 valid_loss: 0.22513 test_loss: 0.11162 \n",
      "Validation loss decreased (inf --> 0.225135).  Saving model ...\n",
      "[  2/200] train_loss: 0.20309 valid_loss: 0.19694 test_loss: 0.08264 \n",
      "Validation loss decreased (0.225135 --> 0.196940).  Saving model ...\n",
      "[  3/200] train_loss: 0.18170 valid_loss: 0.17775 test_loss: 0.07107 \n",
      "Validation loss decreased (0.196940 --> 0.177750).  Saving model ...\n",
      "[  4/200] train_loss: 0.16533 valid_loss: 0.16160 test_loss: 0.06801 \n",
      "Validation loss decreased (0.177750 --> 0.161597).  Saving model ...\n",
      "[  5/200] train_loss: 0.16097 valid_loss: 0.15898 test_loss: 0.06337 \n",
      "Validation loss decreased (0.161597 --> 0.158979).  Saving model ...\n",
      "[  6/200] train_loss: 0.16022 valid_loss: 0.16080 test_loss: 0.06329 \n",
      "[  7/200] train_loss: 0.14983 valid_loss: 0.15092 test_loss: 0.05631 \n",
      "Validation loss decreased (0.158979 --> 0.150919).  Saving model ...\n",
      "[  8/200] train_loss: 0.15451 valid_loss: 0.14392 test_loss: 0.05520 \n",
      "Validation loss decreased (0.150919 --> 0.143917).  Saving model ...\n",
      "[  9/200] train_loss: 0.15431 valid_loss: 0.13891 test_loss: 0.05419 \n",
      "Validation loss decreased (0.143917 --> 0.138909).  Saving model ...\n",
      "[ 10/200] train_loss: 0.14605 valid_loss: 0.14354 test_loss: 0.04907 \n",
      "[ 11/200] train_loss: 0.14152 valid_loss: 0.14502 test_loss: 0.05136 \n",
      "[ 12/200] train_loss: 0.14380 valid_loss: 0.14145 test_loss: 0.04843 \n",
      "[ 13/200] train_loss: 0.14288 valid_loss: 0.13557 test_loss: 0.04863 \n",
      "Validation loss decreased (0.138909 --> 0.135566).  Saving model ...\n",
      "[ 14/200] train_loss: 0.13614 valid_loss: 0.15194 test_loss: 0.04878 \n",
      "[ 15/200] train_loss: 0.13483 valid_loss: 0.13091 test_loss: 0.05156 \n",
      "Validation loss decreased (0.135566 --> 0.130912).  Saving model ...\n",
      "[ 16/200] train_loss: 0.13417 valid_loss: 0.13434 test_loss: 0.04733 \n",
      "[ 17/200] train_loss: 0.13835 valid_loss: 0.13623 test_loss: 0.04818 \n",
      "[ 18/200] train_loss: 0.13251 valid_loss: 0.13691 test_loss: 0.04488 \n",
      "[ 19/200] train_loss: 0.13751 valid_loss: 0.13803 test_loss: 0.04574 \n",
      "[ 20/200] train_loss: 0.13529 valid_loss: 0.14579 test_loss: 0.04808 \n",
      "[ 21/200] train_loss: 0.13024 valid_loss: 0.13076 test_loss: 0.04776 \n",
      "Validation loss decreased (0.130912 --> 0.130756).  Saving model ...\n",
      "[ 22/200] train_loss: 0.13184 valid_loss: 0.14227 test_loss: 0.04365 \n",
      "[ 23/200] train_loss: 0.12781 valid_loss: 0.12899 test_loss: 0.04718 \n",
      "Validation loss decreased (0.130756 --> 0.128989).  Saving model ...\n",
      "[ 24/200] train_loss: 0.12616 valid_loss: 0.13183 test_loss: 0.04508 \n",
      "[ 25/200] train_loss: 0.12998 valid_loss: 0.13575 test_loss: 0.04309 \n",
      "[ 26/200] train_loss: 0.12767 valid_loss: 0.12522 test_loss: 0.04259 \n",
      "Validation loss decreased (0.128989 --> 0.125223).  Saving model ...\n",
      "[ 27/200] train_loss: 0.12226 valid_loss: 0.13255 test_loss: 0.04058 \n",
      "[ 28/200] train_loss: 0.12297 valid_loss: 0.13285 test_loss: 0.04250 \n",
      "[ 29/200] train_loss: 0.12437 valid_loss: 0.14044 test_loss: 0.04345 \n",
      "[ 30/200] train_loss: 0.11868 valid_loss: 0.13696 test_loss: 0.03949 \n",
      "[ 31/200] train_loss: 0.12498 valid_loss: 0.13528 test_loss: 0.03885 \n",
      "[ 32/200] train_loss: 0.11857 valid_loss: 0.13643 test_loss: 0.03872 \n",
      "[ 33/200] train_loss: 0.12382 valid_loss: 0.14743 test_loss: 0.03859 \n",
      "[ 34/200] train_loss: 0.11722 valid_loss: 0.13187 test_loss: 0.04063 \n",
      "[ 35/200] train_loss: 0.11698 valid_loss: 0.12755 test_loss: 0.04064 \n",
      "[ 36/200] train_loss: 0.11636 valid_loss: 0.13385 test_loss: 0.03827 \n",
      "[ 37/200] train_loss: 0.11956 valid_loss: 0.14875 test_loss: 0.03696 \n",
      "[ 38/200] train_loss: 0.11281 valid_loss: 0.12304 test_loss: 0.03848 \n",
      "Validation loss decreased (0.125223 --> 0.123039).  Saving model ...\n",
      "[ 39/200] train_loss: 0.11572 valid_loss: 0.13856 test_loss: 0.03670 \n",
      "[ 40/200] train_loss: 0.11553 valid_loss: 0.13069 test_loss: 0.04137 \n",
      "[ 41/200] train_loss: 0.11655 valid_loss: 0.12695 test_loss: 0.03631 \n",
      "[ 42/200] train_loss: 0.11313 valid_loss: 0.16751 test_loss: 0.03521 \n",
      "[ 43/200] train_loss: 0.11000 valid_loss: 0.12352 test_loss: 0.03551 \n",
      "[ 44/200] train_loss: 0.10762 valid_loss: 0.13898 test_loss: 0.03697 \n",
      "[ 45/200] train_loss: 0.11081 valid_loss: 0.12757 test_loss: 0.03638 \n",
      "[ 46/200] train_loss: 0.11465 valid_loss: 0.13300 test_loss: 0.03758 \n",
      "[ 47/200] train_loss: 0.11318 valid_loss: 0.12042 test_loss: 0.03414 \n",
      "Validation loss decreased (0.123039 --> 0.120422).  Saving model ...\n",
      "[ 48/200] train_loss: 0.10928 valid_loss: 0.12858 test_loss: 0.03510 \n",
      "[ 49/200] train_loss: 0.10444 valid_loss: 0.12580 test_loss: 0.03961 \n",
      "[ 50/200] train_loss: 0.10784 valid_loss: 0.12405 test_loss: 0.03718 \n",
      "[ 51/200] train_loss: 0.10558 valid_loss: 0.12416 test_loss: 0.03523 \n",
      "[ 52/200] train_loss: 0.10836 valid_loss: 0.12545 test_loss: 0.03925 \n",
      "[ 53/200] train_loss: 0.10685 valid_loss: 0.11839 test_loss: 0.03879 \n",
      "Validation loss decreased (0.120422 --> 0.118390).  Saving model ...\n",
      "[ 54/200] train_loss: 0.10903 valid_loss: 0.13122 test_loss: 0.03492 \n",
      "[ 55/200] train_loss: 0.10628 valid_loss: 0.12529 test_loss: 0.03689 \n",
      "[ 56/200] train_loss: 0.10318 valid_loss: 0.12507 test_loss: 0.03945 \n",
      "[ 57/200] train_loss: 0.10983 valid_loss: 0.13352 test_loss: 0.03419 \n",
      "[ 58/200] train_loss: 0.10721 valid_loss: 0.11654 test_loss: 0.03410 \n",
      "Validation loss decreased (0.118390 --> 0.116535).  Saving model ...\n",
      "[ 59/200] train_loss: 0.10855 valid_loss: 0.11260 test_loss: 0.03572 \n",
      "Validation loss decreased (0.116535 --> 0.112603).  Saving model ...\n",
      "[ 60/200] train_loss: 0.10334 valid_loss: 0.12557 test_loss: 0.03657 \n",
      "[ 61/200] train_loss: 0.10480 valid_loss: 0.12540 test_loss: 0.03529 \n",
      "[ 62/200] train_loss: 0.10055 valid_loss: 0.12319 test_loss: 0.03774 \n",
      "[ 63/200] train_loss: 0.10653 valid_loss: 0.12347 test_loss: 0.03953 \n",
      "[ 64/200] train_loss: 0.10808 valid_loss: 0.13620 test_loss: 0.03271 \n",
      "[ 65/200] train_loss: 0.10338 valid_loss: 0.11494 test_loss: 0.03607 \n",
      "[ 66/200] train_loss: 0.10540 valid_loss: 0.12502 test_loss: 0.03537 \n",
      "[ 67/200] train_loss: 0.09538 valid_loss: 0.14371 test_loss: 0.03628 \n",
      "[ 68/200] train_loss: 0.10407 valid_loss: 0.11782 test_loss: 0.03898 \n",
      "[ 69/200] train_loss: 0.10103 valid_loss: 0.14673 test_loss: 0.03255 \n",
      "[ 70/200] train_loss: 0.10426 valid_loss: 0.12858 test_loss: 0.03355 \n",
      "[ 71/200] train_loss: 0.09772 valid_loss: 0.13024 test_loss: 0.03683 \n",
      "[ 72/200] train_loss: 0.10224 valid_loss: 0.12386 test_loss: 0.03732 \n",
      "[ 73/200] train_loss: 0.09837 valid_loss: 0.11745 test_loss: 0.03654 \n",
      "[ 74/200] train_loss: 0.10122 valid_loss: 0.13012 test_loss: 0.03555 \n",
      "[ 75/200] train_loss: 0.09945 valid_loss: 0.12894 test_loss: 0.03179 \n",
      "[ 76/200] train_loss: 0.10002 valid_loss: 0.12252 test_loss: 0.03119 \n",
      "[ 77/200] train_loss: 0.09853 valid_loss: 0.12515 test_loss: 0.03476 \n",
      "[ 78/200] train_loss: 0.09828 valid_loss: 0.12157 test_loss: 0.03410 \n",
      "[ 79/200] train_loss: 0.10069 valid_loss: 0.12607 test_loss: 0.03271 \n",
      "[ 80/200] train_loss: 0.10008 valid_loss: 0.11757 test_loss: 0.03529 \n",
      "[ 81/200] train_loss: 0.09676 valid_loss: 0.12741 test_loss: 0.04464 \n",
      "[ 82/200] train_loss: 0.09947 valid_loss: 0.12818 test_loss: 0.03736 \n",
      "[ 83/200] train_loss: 0.09639 valid_loss: 0.11727 test_loss: 0.03814 \n",
      "[ 84/200] train_loss: 0.09220 valid_loss: 0.13401 test_loss: 0.03387 \n",
      "[ 85/200] train_loss: 0.09336 valid_loss: 0.17758 test_loss: 0.02874 \n",
      "[ 86/200] train_loss: 0.10021 valid_loss: 0.12012 test_loss: 0.03314 \n",
      "[ 87/200] train_loss: 0.09909 valid_loss: 0.14766 test_loss: 0.03640 \n",
      "[ 88/200] train_loss: 0.09350 valid_loss: 0.12127 test_loss: 0.03377 \n",
      "[ 89/200] train_loss: 0.09444 valid_loss: 0.12850 test_loss: 0.03479 \n",
      "[ 90/200] train_loss: 0.09525 valid_loss: 0.13280 test_loss: 0.03475 \n",
      "[ 91/200] train_loss: 0.09656 valid_loss: 0.14104 test_loss: 0.03243 \n",
      "[ 92/200] train_loss: 0.10147 valid_loss: 0.12801 test_loss: 0.02987 \n",
      "[ 93/200] train_loss: 0.09376 valid_loss: 0.13861 test_loss: 0.02742 \n",
      "[ 94/200] train_loss: 0.09617 valid_loss: 0.11967 test_loss: 0.03753 \n",
      "[ 95/200] train_loss: 0.09287 valid_loss: 0.12360 test_loss: 0.03157 \n",
      "[ 96/200] train_loss: 0.09423 valid_loss: 0.13207 test_loss: 0.02953 \n",
      "[ 97/200] train_loss: 0.09140 valid_loss: 0.11714 test_loss: 0.03405 \n",
      "[ 98/200] train_loss: 0.09267 valid_loss: 0.12804 test_loss: 0.03156 \n",
      "[ 99/200] train_loss: 0.09449 valid_loss: 0.13082 test_loss: 0.03356 \n",
      "[100/200] train_loss: 0.09599 valid_loss: 0.13418 test_loss: 0.03223 \n",
      "[101/200] train_loss: 0.09576 valid_loss: 0.13183 test_loss: 0.03355 \n",
      "[102/200] train_loss: 0.09213 valid_loss: 0.12214 test_loss: 0.03141 \n",
      "[103/200] train_loss: 0.09085 valid_loss: 0.12314 test_loss: 0.02976 \n",
      "[104/200] train_loss: 0.08912 valid_loss: 0.11585 test_loss: 0.03424 \n",
      "[105/200] train_loss: 0.09250 valid_loss: 0.14978 test_loss: 0.03774 \n",
      "[106/200] train_loss: 0.09328 valid_loss: 0.13543 test_loss: 0.02823 \n",
      "[107/200] train_loss: 0.09273 valid_loss: 0.16399 test_loss: 0.02925 \n",
      "[108/200] train_loss: 0.09198 valid_loss: 0.15083 test_loss: 0.03089 \n",
      "[109/200] train_loss: 0.09480 valid_loss: 0.16831 test_loss: 0.02973 \n",
      "[110/200] train_loss: 0.09169 valid_loss: 0.12509 test_loss: 0.03777 \n",
      "[111/200] train_loss: 0.08900 valid_loss: 0.12699 test_loss: 0.03697 \n",
      "[112/200] train_loss: 0.08925 valid_loss: 0.12695 test_loss: 0.03087 \n",
      "[113/200] train_loss: 0.08884 valid_loss: 0.12243 test_loss: 0.03200 \n",
      "[114/200] train_loss: 0.09148 valid_loss: 0.12023 test_loss: 0.03317 \n",
      "[115/200] train_loss: 0.09123 valid_loss: 0.12574 test_loss: 0.03368 \n",
      "[116/200] train_loss: 0.09261 valid_loss: 0.12576 test_loss: 0.03117 \n",
      "[117/200] train_loss: 0.08986 valid_loss: 0.13452 test_loss: 0.03419 \n",
      "[118/200] train_loss: 0.08935 valid_loss: 0.12667 test_loss: 0.03535 \n",
      "[119/200] train_loss: 0.08577 valid_loss: 0.12950 test_loss: 0.03520 \n",
      "[120/200] train_loss: 0.08522 valid_loss: 0.13462 test_loss: 0.03083 \n",
      "[121/200] train_loss: 0.08936 valid_loss: 0.14383 test_loss: 0.03049 \n",
      "[122/200] train_loss: 0.08697 valid_loss: 0.12779 test_loss: 0.03252 \n",
      "[123/200] train_loss: 0.09215 valid_loss: 0.14583 test_loss: 0.02675 \n",
      "[124/200] train_loss: 0.09019 valid_loss: 0.13595 test_loss: 0.02682 \n",
      "[125/200] train_loss: 0.09077 valid_loss: 0.12484 test_loss: 0.02800 \n",
      "[126/200] train_loss: 0.09004 valid_loss: 0.12577 test_loss: 0.03257 \n",
      "[127/200] train_loss: 0.09093 valid_loss: 0.12766 test_loss: 0.02843 \n",
      "[128/200] train_loss: 0.08633 valid_loss: 0.12598 test_loss: 0.03304 \n",
      "[129/200] train_loss: 0.08964 valid_loss: 0.13551 test_loss: 0.03075 \n",
      "[130/200] train_loss: 0.08802 valid_loss: 0.12770 test_loss: 0.02956 \n",
      "[131/200] train_loss: 0.08636 valid_loss: 0.10953 test_loss: 0.02987 \n",
      "Validation loss decreased (0.112603 --> 0.109525).  Saving model ...\n",
      "[132/200] train_loss: 0.08673 valid_loss: 0.13458 test_loss: 0.02956 \n",
      "[133/200] train_loss: 0.08793 valid_loss: 0.13299 test_loss: 0.02806 \n",
      "[134/200] train_loss: 0.08524 valid_loss: 0.12013 test_loss: 0.03517 \n",
      "[135/200] train_loss: 0.08724 valid_loss: 0.11902 test_loss: 0.03246 \n",
      "[136/200] train_loss: 0.08625 valid_loss: 0.12148 test_loss: 0.02929 \n",
      "[137/200] train_loss: 0.08373 valid_loss: 0.13032 test_loss: 0.02890 \n",
      "[138/200] train_loss: 0.08579 valid_loss: 0.12159 test_loss: 0.03218 \n",
      "[139/200] train_loss: 0.08421 valid_loss: 0.12414 test_loss: 0.02712 \n",
      "[140/200] train_loss: 0.08917 valid_loss: 0.12749 test_loss: 0.02818 \n",
      "[141/200] train_loss: 0.08701 valid_loss: 0.11563 test_loss: 0.03305 \n",
      "[142/200] train_loss: 0.08495 valid_loss: 0.12255 test_loss: 0.03153 \n",
      "[143/200] train_loss: 0.08551 valid_loss: 0.13440 test_loss: 0.02828 \n",
      "[144/200] train_loss: 0.07946 valid_loss: 0.12241 test_loss: 0.02825 \n",
      "[145/200] train_loss: 0.08406 valid_loss: 0.12883 test_loss: 0.03117 \n",
      "[146/200] train_loss: 0.08576 valid_loss: 0.12654 test_loss: 0.02993 \n",
      "[147/200] train_loss: 0.08018 valid_loss: 0.12266 test_loss: 0.03193 \n",
      "[148/200] train_loss: 0.08672 valid_loss: 0.12260 test_loss: 0.04299 \n",
      "[149/200] train_loss: 0.07936 valid_loss: 0.12560 test_loss: 0.03497 \n",
      "[150/200] train_loss: 0.07936 valid_loss: 0.13128 test_loss: 0.03015 \n",
      "[151/200] train_loss: 0.08736 valid_loss: 0.11692 test_loss: 0.02981 \n",
      "[152/200] train_loss: 0.08123 valid_loss: 0.11245 test_loss: 0.03177 \n",
      "[153/200] train_loss: 0.08563 valid_loss: 0.11802 test_loss: 0.02637 \n",
      "[154/200] train_loss: 0.08380 valid_loss: 0.12696 test_loss: 0.02905 \n",
      "[155/200] train_loss: 0.08263 valid_loss: 0.13674 test_loss: 0.03255 \n",
      "[156/200] train_loss: 0.08316 valid_loss: 0.12823 test_loss: 0.02889 \n",
      "[157/200] train_loss: 0.07873 valid_loss: 0.11980 test_loss: 0.02766 \n",
      "[158/200] train_loss: 0.07678 valid_loss: 0.13732 test_loss: 0.02825 \n",
      "[159/200] train_loss: 0.08384 valid_loss: 0.14230 test_loss: 0.02666 \n",
      "[160/200] train_loss: 0.08145 valid_loss: 0.13109 test_loss: 0.02915 \n",
      "[161/200] train_loss: 0.08204 valid_loss: 0.13306 test_loss: 0.02874 \n",
      "[162/200] train_loss: 0.07939 valid_loss: 0.13244 test_loss: 0.02903 \n",
      "[163/200] train_loss: 0.07934 valid_loss: 0.11625 test_loss: 0.03113 \n",
      "[164/200] train_loss: 0.07799 valid_loss: 0.12816 test_loss: 0.02972 \n",
      "[165/200] train_loss: 0.08236 valid_loss: 0.13409 test_loss: 0.03329 \n",
      "[166/200] train_loss: 0.07789 valid_loss: 0.12133 test_loss: 0.03165 \n",
      "[167/200] train_loss: 0.08201 valid_loss: 0.12213 test_loss: 0.03100 \n",
      "[168/200] train_loss: 0.08334 valid_loss: 0.13778 test_loss: 0.03210 \n",
      "[169/200] train_loss: 0.08130 valid_loss: 0.12599 test_loss: 0.03144 \n",
      "[170/200] train_loss: 0.07884 valid_loss: 0.15966 test_loss: 0.02670 \n",
      "[171/200] train_loss: 0.08001 valid_loss: 0.12272 test_loss: 0.03472 \n",
      "[172/200] train_loss: 0.08243 valid_loss: 0.13325 test_loss: 0.02773 \n",
      "[173/200] train_loss: 0.07840 valid_loss: 0.12915 test_loss: 0.03096 \n",
      "[174/200] train_loss: 0.08123 valid_loss: 0.12453 test_loss: 0.02798 \n",
      "[175/200] train_loss: 0.07991 valid_loss: 0.12701 test_loss: 0.02685 \n",
      "[176/200] train_loss: 0.08377 valid_loss: 0.14564 test_loss: 0.02748 \n",
      "[177/200] train_loss: 0.08157 valid_loss: 0.12502 test_loss: 0.03230 \n",
      "[178/200] train_loss: 0.07930 valid_loss: 0.15469 test_loss: 0.02893 \n",
      "[179/200] train_loss: 0.07943 valid_loss: 0.11413 test_loss: 0.03463 \n",
      "[180/200] train_loss: 0.07480 valid_loss: 0.11702 test_loss: 0.02947 \n",
      "[181/200] train_loss: 0.07564 valid_loss: 0.11523 test_loss: 0.02935 \n",
      "[182/200] train_loss: 0.07918 valid_loss: 0.12038 test_loss: 0.03136 \n",
      "[183/200] train_loss: 0.07988 valid_loss: 0.13077 test_loss: 0.02458 \n",
      "[184/200] train_loss: 0.07842 valid_loss: 0.10851 test_loss: 0.03136 \n",
      "Validation loss decreased (0.109525 --> 0.108511).  Saving model ...\n",
      "[185/200] train_loss: 0.08283 valid_loss: 0.11080 test_loss: 0.03541 \n",
      "[186/200] train_loss: 0.07358 valid_loss: 0.11039 test_loss: 0.02773 \n",
      "[187/200] train_loss: 0.07961 valid_loss: 0.12404 test_loss: 0.02886 \n",
      "[188/200] train_loss: 0.07952 valid_loss: 0.13783 test_loss: 0.02741 \n",
      "[189/200] train_loss: 0.08058 valid_loss: 0.12590 test_loss: 0.03625 \n",
      "[190/200] train_loss: 0.07694 valid_loss: 0.12376 test_loss: 0.02837 \n",
      "[191/200] train_loss: 0.07776 valid_loss: 0.11828 test_loss: 0.02992 \n",
      "[192/200] train_loss: 0.07437 valid_loss: 0.12964 test_loss: 0.02493 \n",
      "[193/200] train_loss: 0.07772 valid_loss: 0.13264 test_loss: 0.02757 \n",
      "[194/200] train_loss: 0.07879 valid_loss: 0.11689 test_loss: 0.02950 \n",
      "[195/200] train_loss: 0.07873 valid_loss: 0.12064 test_loss: 0.03007 \n",
      "[196/200] train_loss: 0.07589 valid_loss: 0.12205 test_loss: 0.02780 \n",
      "[197/200] train_loss: 0.07530 valid_loss: 0.12417 test_loss: 0.02932 \n",
      "[198/200] train_loss: 0.07964 valid_loss: 0.12802 test_loss: 0.03046 \n",
      "[199/200] train_loss: 0.07225 valid_loss: 0.11674 test_loss: 0.02498 \n",
      "[200/200] train_loss: 0.07313 valid_loss: 0.12609 test_loss: 0.02569 \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.net import PTPNet\n",
    "from utils.training import train_model\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 200\n",
    "\n",
    "train_loader = dl_train\n",
    "valid_loader = dl_valid\n",
    "test_loader = dl_test\n",
    "\n",
    "model = PTPNet(1,1,32).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5.E-5)\n",
    "#criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([2.9])).cuda()\n",
    "fn = '../data/refit/network_weights/REFIT_washingmachine_network_weights.pth'\n",
    "model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn, train_loader, valid_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9434991-37cf-4d74-8257-3858a7c7699c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf0c23-44b0-4b56-95e9-734318a25bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
